# Feature Specification: MixSeek-Core Multi-Agent Framework

**Feature Branch**: `001-mixseek-core-specs`
**Created**: 2025-10-14
**Status**: Draft
**Input**: User description: "mixseek-coreの設計に関する要件定義を行ってください、不明な点は必ずヒアリングしてください

やること: 設計のコア部分
やらないこと: 細部の設計、実装部分

エージェントワークフロー詳細は次を参照:
assets/workflow.png

用語集: ../glossary.md

mixseek-coreはユーザから受け取ったプロンプトを [TUMIX](https://arxiv.org/html/2510.01279v1) を参考にしたアルゴリズムで動作するエージェントで、Submissionをユーザに返します
エージェントはチーム単位で構成され、チームリーダーとなるLeader AgentはMember Agent(目的に特化したエージェント)に指示を出します
Leader Agent->各Member Agentの処理は繰り返し行われ、この繰り返しの単位をRoundといい、Round ControllerはこのRoundを管理します
Roundは終了判定の条件に合致したら打ち切られ、Submissionを返します

Orchestration Layerはのチームを管理するレイヤーで、複数チームが設定された場合はこのレイヤーが実行管理などを行います
SubmissionはEvaluatorによってスコアリングされ、Leader Boardに登録されます"

## User Scenarios & Testing

### User Story 1 - 基本的な分析タスクの実行 (Priority: P1)

ユーザが複数の専門領域（データ分析、Web検索、推論など）を必要とする複雑な分析プロンプトを送信します。システムは並列で動作する複数の専門チームを通じてこれを処理し、各チームが最適解を見つけるまで複数のラウンドで改善を行います。

**Why this priority**: これがコアバリューである多様な専門エージェントチームによる協調的な複雑分析の実現です。これがなければシステムの存在意義がありません。

**Independent Test**: 複雑なプロンプトを送信し、多エージェント協調と反復改善を示す高品質のSubmissionを受け取ることで完全にテスト可能です。

**Acceptance Scenarios**:

1. **Given** ユーザが複雑な分析プロンプトを送信する、**When** システムが処理を開始する、**Then** 複数のチームが自動作成され並列作業を開始する
2. **Given** チームがプロンプトに取り組んでいる、**When** 各チームがラウンドを完了する、**Then** 評価フィードバックを受け取り次のラウンドでアプローチを改善できる
3. **Given** チームが分析を完了する、**When** 全チームがラウンドを終える、**Then** ユーザは最高スコアのSubmissionを最終結果として受け取る

---

### User Story 2 - マルチチーム競合による品質向上 (Priority: P2)

同一のユーザプロンプトに対して異なるチーム構成とアプローチを持つ複数のチームが同時に取り組みます。各チームは複数のラウンドを通じて解決策を反復的に改善し、評価スコアに基づいて最優秀のSubmissionを生み出すために競合します。

**Why this priority**: TUMIXに着想を得た競合改善メカニズムを実現し、単一チームアプローチよりも優れた結果をもたらします。

**Independent Test**: 一つのプロンプトに対して複数チームを設定し、それらが独立して動作しながら最優秀スコアを競うことで検証可能です。

**Acceptance Scenarios**:

1. **Given** タスクに複数のチームが設定されている、**When** それらが同時実行される、**Then** 各チームは干渉なく独立したワークフローを維持する
2. **Given** チームがSubmissionを完了する、**When** 評価が実行される、**Then** スコアが比較され最優秀のSubmissionが選択される
3. **Given** チームが競合している、**When** あるチームが著しく優れたスコアを出す、**Then** そのSubmissionが最終結果で優先される

---

### User Story 3 - パフォーマンス監視と最適化 (Priority: P3)

ユーザと管理者は詳細なロギングと可視化インターフェースを通じて、チームパフォーマンス、ラウンド進行状況、システムヘルスを監視できます。これにより、どのチーム構成と戦略が最も効果的かを理解できます。

**Why this priority**: システムの透明性と最適化に不可欠ですが、コア機能の後の優先度です。

**Independent Test**: 分析タスクを実行し、すべての重要な指標、ログ、可視化が正しく取得・表示されることで検証可能です。

**Acceptance Scenarios**:

1. **Given** チームがラウンドを実行中である、**When** ユーザが監視インターフェースにアクセスする、**Then** リアルタイムの進行状況とパフォーマンス指標を確認できる
2. **Given** 履歴データが利用可能である、**When** 管理者がシステムパフォーマンスを確認する、**Then** 成功するチーム構成のパターンを特定できる
3. **Given** システム診断が動作している、**When** 問題が発生する、**Then** 詳細なログとアラートが根本原因の特定に役立つ

---

### Edge Cases

- チームが最大ラウンド制限内で有効なSubmissionを生成できない場合はどうなるか？
- Member Agentが応答しない、または無効な出力を生成する場合、システムはどう対処するか？
- Evaluatorのスコアリングで複数チーム間でタイが発生した場合はどうするか？
- 多数のチームが同時実行されている際にリソース制約をどう管理するか？
- Round Controllerがラウンド遷移の管理に失敗した場合はどうするか？

## Requirements

### Functional Requirements

- **FR-001**: システムはユーザプロンプトを受け取り、ユーザが明示的に指定したチーム構成に基づいてチームを作成しなければならない
- **FR-002**: システムは同一ユーザプロンプトに取り組む複数チームの並列実行をサポートしなければならない
- **FR-003**: チームは複数の専門Member Agentを調整する1名のLeader Agentで構成されなければならない
- **FR-004**: Leader Agentはタスク分解、Member Agentへの作業割当、出力統合ができなければならない
- **FR-005**: Member Agentは特定ドメイン（データ分析、Web検索、推論等）に特化しなければならない。Member Agentには以下の2種類が存在する:
  - **a) システム標準Member Agent**: mixseek-coreパッケージに組み込まれた標準エージェント。TOMLファイルで名前を指定するのみで利用可能(例: data-analyst, web-searcher, reasoning-agent)
  - **b) ユーザー作成Member Agent**: ユーザがmixseek-core SDKで実装するカスタムエージェント。TOMLファイルでPythonモジュールとクラスを指定。特定データソースへの接続やドメイン特化機能を独自開発可能
  - **共通の実装パターン**: すべてのMember Agent(システム標準・ユーザー作成)は`BaseMemberAgent`基底クラスを継承し、`execute(task, context)`メソッドを実装。Leader AgentからPydantic AI Toolsetを通じて呼び出され、同一プロセス内で実行される(高速、低オーバーヘッド)
  - **将来実装**: 現在はToolsetベースの実装を標準とし、将来的に高セキュリティが必要な場合にMCPサーバーモードを追加予定(オプション)
- **FR-006**: システムはチームがSubmissionを改善する反復的なラウンドベース処理を実装しなければならない。各ラウンドでLeader Agentが実行されSubmissionを生成し、Evaluatorがフィードバックを提供する。次ラウンドではコンテキストをクリアな状態で開始し、引き継ぐのは前ラウンドのSubmissionの結果と評価フィードバックの両方とする。評価フィードバックを新しいユーザープロンプトとして追加することで、反復による段階的な品質改善を実現する。各ラウンドのMessage HistoryはJSON形式でデータベースに保存され、完了したラウンドの詳細履歴はアーカイブテーブルに直接保存される
- **FR-007**: Round Controllerはラウンドライフサイクル、Submission収集、評価調整を管理しなければならない。管理対象には、ラウンド番号、現在のSubmission、評価スコアとフィードバック、各ラウンドのMessage Historyが含まれる。Message History永続化として、SQLiteまたはPostgreSQLの専用テーブルに保存する。各ラウンド終了時にMessage HistoryをJSON形式でシリアライズして保存し、完了したラウンドの詳細履歴はアーカイブテーブルに直接保存する。次ラウンド開始時には新しいクリーンなコンテキストで開始し、前ラウンドのSubmission結果と評価フィードバックのみを引き継ぐ
- **FR-008**: システムは複数の評価指標を使用してSubmissionを評価し、ユーザがカスタマイズ可能でなければならない。評価器を配列として1つ以上設定可能とし、サポートする評価器タイプとして、a) LLM-as-a-Judge(Pydantic AI Agentを使用した柔軟な評価)、b) カスタム評価関数(Pythonコードで実装したルールベース評価、決定論的な評価が必要な場合に使用)を提供する。設定方法はTOMLファイルで評価器の組み合わせと基準を指定可能とする。複数評価器による総合評価の算出方法（重み付け、平均計算等）は本設計のスコープ外とする
- **FR-009**: Evaluatorは定量的スコアと定性的フィードバックコメントの両方を提供しなければならない。出力は`EvaluationResult` Pydantic Modelで型安全に保証される
- **FR-010**: システムはチームパフォーマンスを追跡しSubmissionをランキングするLeader Boardをデータベースで維持し、ダッシュボードで閲覧可能にしなければならない（同スコア時は同順位として扱う）
- **FR-011**: システムはラウンド終了のためのインターフェースを提供しなければならない。最大ラウンド数に到達した場合は、その時点での最高スコアSubmissionを最終結果として返す。具体的な終了判定ロジックおよび終了条件の実装は本設計のスコープ外とする
- **FR-012**: Orchestration Layerは階層的なリソース管理を実装し、リソース割当、タイムアウト処理、チームライフサイクルを管理しなければならない。多層リソース管理アーキテクチャとして、以下の4層を実装する: 1) ラン単位制限(Pydantic AI UsageLimitsによる1回のagent.run()呼び出しに対する制限、超過時は警告とリトライ、リトライ失敗でラウンド失敗)、2) ラウンド単位累積制限(1ラウンド内のLeader Agent + Member Agent呼び出し + Evaluatorの累積制限、超過時は警告後ラウンド失敗)、3) チーム単位累積制限(全ラウンドを通じたチーム全体のリソース制限、トークン数・リクエスト数・実行時間を含む、超過時は警告後チーム失格)、4) システム全体制限(並列実行される全チーム合計のリソース制限、同時実行チーム数上限を含む、超過時は新規受付停止)。同時実行チーム数の上限はTOMLファイルで設定可能とする。非LLMリソース(CPU時間、カスタムツールコスト)も管理対象とする
- **FR-013**: システムはチーム状況、ラウンド進行、パフォーマンス指標の監視インターフェースを提供しなければならない。具体的な監視・ログの実装は本設計のスコープ外とする
- **FR-014**: システムは異なるチーム構成とエージェント設定をサポートしなければならない。設定はTOMLファイルで階層的に定義される。階層構造は、チーム全体設定(トップレベル: name, description, max_rounds, member_agent_limit)、Leader Agent設定(model, instructions)、Member Agent設定(複数定義可能、システム標準とユーザー作成の両方)、Evaluator設定(type, criteria)で構成される。各Member Agentでは、名前、タイプ(system/custom)、モデル、capabilities、descriptionを定義する。Pydantic Modelで設定構造を検証し、階層的な設定の妥当性チェックと必須フィールドの存在確認を行う
- **FR-015**: システムは最高スコアのSubmissionをユーザへの最終出力として返さなければならない
- **FR-016**: システムは分析用データアクセスインターフェースを提供しなければならない。具体的なログ実装は本設計のスコープ外とする
- **FR-017**: システムは全体タスク実行を損なうことなく障害を適切に処理しなければならない。Member Agent（システム標準・カスタム問わず）またはEvaluatorでエラーが発生した場合、即座に該当チーム全体を失格とし、他チームの結果のみで評価を継続する。エラー情報の記録方法は本設計のスコープ外とする
- **FR-018**: VerificatorはユーザMember Agent設定のTOML構文チェック、基本妥当性検証機能、及びセキュリティ診断を含む完全な品質保証テストを提供しなければならない。品質保証テストには、接続確認、応答性テスト、機能検証、セキュリティ脆弱性診断、宣言されたCapabilityの検証が含まれる。登録プロセスは：ユーザTOMLファイル作成 → Verificator検証（構文・妥当性・品質保証・セキュリティテスト） → システム読み込み → チーム割り当て可能化の順序で実行される
- **FR-019**: Leader AgentとMember Agent間の通信は、Pydantic AI Toolsetを通じた直接呼び出しで行う。現在の標準実装では、すべてのMember Agent(システム標準・ユーザー作成)はToolsetとして登録され、Leader Agentと同一プロセス内で実行される。関数呼び出しによる高速な通信を実現する。将来的には、高セキュリティが必要な特定のエージェントのみ、MCPプロトコルをサポートし、別プロセスで実行して厳格な分離を実現できるオプションを追加予定
- **FR-020**: Member Agentのクラス設計は、ユーザがSDKでカスタムMember Agentを開発できるよう拡張可能でなければならない（TOMLファイル設定と組み合わせて使用）
- **FR-021**: システム(mixseek-core)はPythonパッケージで提供され、PyPIで配布可能な形式でなければならない
- **FR-022**: Submissionの構造は、Pydantic Modelを使用して型安全に定義されなければならない。必須フィールドとして`content`(Submissionの主要コンテンツ)を持ち、デフォルト値を持つフィールドとして`format`(markdown/json/csv/html、デフォルトmarkdown)と`metadata`(追加メタデータ、デフォルト空dict)を持つ。自動生成フィールドとして`generated_at`(生成日時、システムが自動設定)、`team_id`(チームの一意識別子、Round Controllerが設定)、`team_name`(生成したチーム名、Round Controllerが設定)、`round_number`(生成されたラウンド番号、Round Controllerが設定)を持つ。Pydanticが自動的に型チェックとバリデーションを実行し、Leader Agentが生成に失敗した場合はPydantic AI ReflectionがValidationErrorをLLMにフィードバックして自動的に再試行する(最大リトライ回数は設定可能)

### Key Entities

- **User Prompt**: タスク定義、制約、期待する出力形式、成功基準を含む入力仕様
- **Team**: 1名のLeader Agentと複数の専門Member Agentが協働する協調単位
- **Leader Agent**: タスク分解、作業割当、進捗管理、出力統合の責任を負う調整者。TOMLファイルで機能・専門分野を設定可能
- **Member Agent**: 特定ドメイン（データ取得、分析、推論、検索等）に特化した専門作業者。システム標準型とユーザー作成型の両方が存在する。システム標準型はTOMLファイルで設定、ユーザー作成型はTOMLファイルでの設定定義とSDKによる実装コード開発の両方が必要。すべてのMember Agentは`BaseMemberAgent`基底クラスを継承し、現在はPydantic AI Toolsetを通じて同一プロセス内で実行される。将来的にはオプションでMCPプロトコルによる別プロセス実行をサポート予定
- **Submission**: チームが生成する最終出力で、Pydantic Modelで型安全に定義される。必須フィールド(content)、デフォルト値を持つフィールド(format, metadata)、自動生成フィールド(generated_at, team_id, team_name, round_number)で構成される
- **Round**: チームが分析し、Submissionを生成し、評価フィードバックを受け、アプローチを改善する完全サイクル。各ラウンドはクリーンなコンテキストで開始し、前ラウンドのSubmission結果と評価フィードバックのみを引き継ぐ
- **Round Controller**: 特定チームのラウンドライフサイクル、評価調整、終了判定を管理するインスタンス。ラウンド番号、Submission、評価スコアとフィードバック、Message Historyを管理し、データベースに永続化する
- **Evaluator**: 複数の評価基準を使用してSubmissionの定量的スコアリングと定性的フィードバックを提供するコンポーネント。LLM-as-a-Judge、カスタム評価関数の評価器を配列として1つ以上組み合わせ可能で、出力は`EvaluationResult` Pydantic Modelで型安全に保証される。複数評価器による総合評価の算出方法は機能別設計に委譲される
- **Leader Board**: 全チーム・ラウンドにわたるSubmission、スコア、コメント、ランキングをデータベースで管理し、ダッシュボードなどのビューでユーザが閲覧できるシステム
- **Orchestration Layer**: チーム作成、リソース割当、並列実行、最終結果選択を処理する管理システム。階層的なリソース管理(ラン単位、ラウンド単位、チーム単位、システム全体)を実装する
- **RoundState**: 各ラウンドの完全な状態(Submission、評価スコア、フィードバック、Message History)を保持するdataclass。データベースに永続化され、長期実行時のメモリ負荷を削減する
- **EvaluationResult**: Evaluatorの出力を定義するPydantic Model。定量的スコアと定性的フィードバックを型安全に提供する
- **MemberAgentConfig**: TOMLファイルから読み込まれるMember Agent設定を表すPydantic Model。Verificatorによる検証に使用される

## Clarifications

### Session 2025-10-14

- Q: MixSeek-Coreの技術基盤について、dseek-ossとの関係はどうするか？ → A: 完全に独立した新規技術スタックで構築
- Q: Evaluatorの定量的・定性的評価の組み合わせ方法と重み付けはどうするか？ → A: ユーザ設定(TOMLファイル)で指定可能にする
- Q: Leader Boardでの同スコア時の順位決定はどうするか？ → A: 同スコアの場合は同順位として扱い、最高スコアチームが複数存在することを許可
- Q: Member Agentの障害・応答なし時のチーム対応はどうするか？ → A: 障害発生時は該当チーム全体を失格とし、他のチームの結果のみで評価
- Q: Member Agentの数の上限設定について → A: チーム内のMember Agent数の上限はTOMLファイルで設定可能にする（論文参考値は15名）
- Q: ユーザがカスタムMember Agentを作成してシステムに統合する方法はどうしますか？ → A: 全てのMember AgentはTOMLファイルで管理されるため、ユーザカスタムエージェントもTOML設定で登録
- Q: カスタムMember Agentの登録プロセスはどのような手順で行われますか？ → A: ユーザがTOMLファイル作成 → Verificatorが構文/構造を検証 → システムがエージェント定義を読み込み → チーム割り当て可能になる
- Q: カスタムMember Agentが実行時に障害を起こした場合の処理方法は？ → A: タイムアウト/エラー検出 → 該当チーム失格 → 他チームで処理継続 → ログ記録
- Q: FR-018のエージェント権限管理をどう扱いますか？ → A: FR-018を削除し、ユーザ作成Member Agent側でのみ権限管理（仕様スコープ外）
- Q: ユーザ作成のMember AgentがTOMLファイルで登録される際、システムはどのようにそれらの機能を認識しますか？ → A: TOMLファイル内で各エージェントの機能・専門分野を明示的に定義
- Q: ユーザ作成のMember Agentの品質や妥当性をシステムはどのように保証しますか？ → A: VerificatorにTOMLの構文チェック機能などを持たせ、必要最小限のチェック機能はシステム側も持つ
- Q: Leader AgentとユーザMember Agentの通信はどのような方式で行いますか？ → A: Pydantic AI Toolsetを通じた直接呼び出し（高速、低オーバーヘッド、標準実装）。将来的にはオプションでMCPプロトコルをサポート予定

### Session 2025-10-15

- Q: カスタムMember Agentの開発スコープはどの範囲までを想定しますか？ → A: 拡張スコープ：ユーザが独自開発可能（特定データ接続・ドメイン特化含む）、将来的にSDK提供を視野
- Q: SDK提供の具体的スコープと優先度はどの程度ですか？ → A: SDKは将来的な可能性として記載のみ（具体的計画なし）
- Q: Member Agentクラスの拡張性要件はどう定義しますか？ → A: Member Agentのクラス設計は、将来ユーザがSDKでカスタムMember Agentを開発できるよう拡張可能でなければならない
- Q: FR-011の終了判定ロジックの実装戦略はどうするか？ → A: 終了判定のロジック自体を本コア設計のスコープ外とする
- Q: Member Agentの通信エラーハンドリング戦略はどうするか？ → A: Member Agentエラー時は即座にチーム全体を失格とし、他チームで処理を継続
- Q: リソース管理とスケーラビリティ制約はどう扱うか？ → A: スケーリングの設計は本コア設計のスコープ外とする
- Q: チーム構成の自動決定戦略はどうするか？ → A: ユーザが明示的にチーム構成を指定する必要があり、自動決定は行わない
- Q: 監視・ログ機能の実装範囲はどうするか？ → A: ログの設計は本コア設計のスコープ外とする
- Q: FR-018 Verificatorの責務にエージェント動作妥当性の診断機能を含めるか？ → A: エージェント動作妥当性の診断機能を追加する
- Q: Verificatorの診断機能の範囲について → A: セキュリティ診断を含む完全な品質保証テストを実施
- Q: FR-014にLeader Agentの設定も含めるか？ → A: Leader Agentの設定も含める
- Q: ユーザ作成型Member AgentはTOMLファイル以外にもSDKによる開発が必要か？ → A: TOMLファイル設定とSDKによる実装コード開発の両方が必要
- Q: Leader Boardはリポジトリではなくデータベース管理にすべきか？ → A: データベースで管理され、ダッシュボードビューでユーザが閲覧できる方式に変更
- Q: FR-008の評価指標カスタマイズにおける具体的な調整内容のスコープはどうするか？ → A: 重み比などの具体的な調整内容はコア設計のスコープ外とする
- Q: システム(mixseek-core)の配布形式要件を追加すべきか？ → A: PythonパッケージでPyPI配布可能な形式で提供する

### Session 2025-10-21

- Q: 「Phase」や「フェーズ」という用語の使用について → A: これらの用語を使用せず、「現在の標準実装」「将来的に追加予定」のような表現に統一する
- Q: ラウンド間でのMessage Historyの引き継ぎ方針について → A: 次ラウンドではコンテキストをクリアな状態で開始し、引き継ぐのは前ラウンドのSubmissionの結果と評価フィードバックのみとする。評価フィードバックを新しいユーザープロンプトとして追加することで反復改善を実現する
- Q: FR-006とFR-007間でのラウンド継承情報の不整合について → A: FR-006を更新して、Submission結果と評価フィードバックの両方が継承されることを明示的に記述する（FR-007と整合させる）
- Q: 同時実行チーム数の制限について → A: 同時実行チーム数の上限をTOMLファイルで設定可能にする
- Q: Member Agentの通信実装における不整合について → A: FR-019のPydantic AI Toolset実装を標準とし、ClarificationsのMCP記述を更新する
- Q: 最大ラウンド数とタイムアウト処理について → A: 最大ラウンド数到達時にその時点での最高スコアSubmissionを返す
- Q: Evaluatorの評価タイムアウトと失敗処理について → A: Evaluator失敗時にチーム全体を失格とする
- Q: Message History永続化における「移動」の実装方式について → A: ラウンド完了時にアーカイブテーブルに直接保存（移動ではなく保存）
- Q: 階層的リソース制限とエスカレーション動作のマッピングについて → A: ラン単位制限超過→警告+リトライ、ラウンド単位累積制限超過→警告+ラウンド失敗、チーム単位累積制限超過→警告+チーム失格、システム全体制限超過→新規受付停止
- Q: FR-023のconfidence_scoreのスコープについて → A: confidence_scoreを完全に削除し、Submissionモデルから除外する
- Q: Submissionモデルへのteam_id追加について → A: team_idを自動生成フィールドとして追加し、Round Controllerが設定する
- Q: Evaluatorの評価方式設計について → A: 評価器を配列として設定可能にし、総合評価の算出方法は機能別設計に委譲してスコープ外とする
- Q: TeamDependenciesのスコープについて → A: TeamDependenciesをコア設計のスコープ外とし、FR-022を削除してKey Entitiesからも除外する

## Success Criteria

### Measurable Outcomes

- **SC-001**: 複雑な分析タスクにおいて、チームが単一エージェントアプローチより少なくとも20%高いスコアのSubmissionを生成する
- **SC-002**: システムがユーザプロンプトを処理し、合理的な時間制限内（典型的な分析タスクで10分未満）で最終結果を返す
- **SC-003**: マルチチーム競合が単一チーム実行より測定可能に優れた結果を生む（少なくとも15%のスコア向上）
- **SC-004**: ラウンドベース反復が最適終了まで各ラウンドで明確な改善傾向を示す
- **SC-005**: 同時チーム実行中にシステムが高可用性（>95%）を維持する
- **SC-006**: 評価スコアリングが一貫性と信頼性を実証する（人間判定者との評価者間信頼性>0.8）
- **SC-007**: 並列チーム実行でリソース使用効率が維持される（チーム数に対する線形スケーリング）
- **SC-008**: 最終Submission品質に対するユーザ満足度が80%を超える肯定的フィードバック
- **SC-009**: システム診断機能が監視インターフェース内で90%の運用問題を特定・解決可能にする

## Assumptions

- チームは1名のLeader Agentと複数のMember Agentで構成され、Member Agent数の上限はTOMLファイルで設定される（論文参考では最大15名）
- ラウンド処理の終了判定は外部実装により決定される
- 評価システムは複数の指標を使用し、ユーザがカスタマイズ可能である
- 複数の評価基準には精度、完全性、関連性、形式準拠が含まれ得るが、具体的な調整方法は外部実装により決定される
- チーム構成（Leader AgentとMember Agentの組み合わせ）はユーザーがTOMLファイルで事前設定する
- リソース制約の管理は外部実装により決定される
- Member Agentの専門化にはWeb検索、データ分析、推論、ドメイン固有知識などの標準ドメインが含まれる
- ユーザプロンプトはSubmission構造を導くため期待する出力形式を指定する
- フレームワークは独自のプラグイン基盤を構築し、外部ツールアクセスを提供する
- システムはPythonエコシステムと互換性を持ち、標準的なパッケージ管理ツール（pip）でインストール可能である