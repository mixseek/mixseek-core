# Implementation Tasks: MixSeek-Core Orchestrator

**Feature**: MixSeek-Core Orchestrator - ãƒãƒ«ãƒãƒãƒ¼ãƒ å”èª¿å®Ÿè¡Œ
**Branch**: `025-mixseek-core-orchestration`
**Spec**: [spec.md](./spec.md)
**Plan**: [plan.md](./plan.md)

**âš ï¸ IMPORTANT NOTE (2025-11)**:
ã“ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã¯æ­´å²çš„ãªå®Ÿè£…ã‚¿ã‚¹ã‚¯ãƒªã‚¹ãƒˆã§ã™ã€‚ä»¥ä¸‹ã®ã‚¯ãƒ©ã‚¹/é–¢æ•°ã¯**Feature 101ã§å»ƒæ­¢ãƒ»å‰Šé™¤ã•ã‚Œã¾ã—ãŸ**ï¼š
- `OrchestratorConfig` â†’ `OrchestratorSettings`ã«çµ±åˆ
- `TeamReference` â†’ `OrchestratorSettings.teams`ã«çµ±åˆ
- `load_orchestrator_config()` â†’ `load_orchestrator_settings()`ã«ãƒªãƒãƒ¼ãƒ 

ç¾åœ¨ã®å®Ÿè£…ã«ã¤ã„ã¦ã¯`specs/016-round-config/`ã‚’å‚ç…§ã—ã¦ãã ã•ã„ã€‚

ã“ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã¯ã€TDDï¼ˆTest-Driven Developmentï¼‰ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã«åŸºã¥ã„ãŸå®Ÿè£…ã‚¿ã‚¹ã‚¯ãƒªã‚¹ãƒˆã§ã™ã€‚

## Task Execution Strategy

- **Test-First**: Article 3ã«å¾“ã„ã€ã™ã¹ã¦ã®å®Ÿè£…ã‚¿ã‚¹ã‚¯ã®å‰ã«ãƒ†ã‚¹ãƒˆã‚’ä½œæˆ
- **User Storyé †**: P1 â†’ P2 â†’ P3 ã®å„ªå…ˆé †ä½ã§å®Ÿè£…
- **ä¸¦åˆ—å¯èƒ½ã‚¿ã‚¹ã‚¯**: `[P]`ãƒãƒ¼ã‚«ãƒ¼ã§æ˜ç¤º
- **Independent Testing**: å„User Storyã¯ç‹¬ç«‹ã—ã¦ãƒ†ã‚¹ãƒˆå¯èƒ½

## Phase 1: Setup & Infrastructure

### T001: ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆæ§‹é€ ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—
**File**: `src/mixseek/orchestrator/__init__.py`
**Description**: orchestratorãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã¨ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸åˆæœŸåŒ–ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆ

```python
# src/mixseek/orchestrator/__init__.py
"""MixSeek-Core Orchestrator - ãƒãƒ«ãƒãƒãƒ¼ãƒ å”èª¿å®Ÿè¡Œ"""

from mixseek.orchestrator.models import (
    ExecutionSummary,
    OrchestratorTask,
    RoundResult,
    TeamStatus,
)
from mixseek.orchestrator.orchestrator import Orchestrator, load_orchestrator_settings
from mixseek.orchestrator.round_controller import RoundController

__all__ = [
    "Orchestrator",
    "RoundController",
    "OrchestratorTask",
    "TeamStatus",
    "RoundResult",
    "ExecutionSummary",
    "load_orchestrator_settings",
]
```

**Dependencies**: ãªã—
**Checkpoint**: ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸æ§‹é€ ãŒæ­£ã—ãä½œæˆã•ã‚Œã¦ã„ã‚‹

---

### T002: ãƒ†ã‚¹ãƒˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—
**File**: `tests/unit/orchestrator/__init__.py`, `tests/integration/test_orchestrator_e2e.py`
**Description**: ãƒ†ã‚¹ãƒˆç”¨ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã¨ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆ

```bash
mkdir -p tests/unit/orchestrator
touch tests/unit/orchestrator/__init__.py
touch tests/unit/orchestrator/test_models.py
touch tests/unit/orchestrator/test_orchestrator.py
touch tests/unit/orchestrator/test_round_controller.py
touch tests/integration/test_orchestrator_e2e.py
```

**Dependencies**: ãªã—
**Checkpoint**: ãƒ†ã‚¹ãƒˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªæ§‹é€ ãŒä½œæˆã•ã‚Œã¦ã„ã‚‹

---

## Phase 2: Foundational Models (Blocking Prerequisites)

ã“ã‚Œã‚‰ã®ãƒ¢ãƒ‡ãƒ«ã¯ã™ã¹ã¦ã®User Storyã§ä½¿ç”¨ã•ã‚Œã‚‹ãŸã‚ã€æœ€åˆã«å®Ÿè£…ã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚

### T003 [P]: [US1] OrchestratorTask ãƒ¢ãƒ‡ãƒ«ã®ãƒ†ã‚¹ãƒˆä½œæˆ
**Story**: User Story 1 - ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆå—ä¿¡ã¨ãƒãƒ«ãƒãƒãƒ¼ãƒ èµ·å‹•
**File**: `tests/unit/orchestrator/test_models.py`
**Description**: OrchestratorTaskãƒ¢ãƒ‡ãƒ«ã®ãƒ¦ãƒ‹ãƒƒãƒˆãƒ†ã‚¹ãƒˆã‚’ä½œæˆï¼ˆTDD: Red phaseï¼‰

```python
import pytest
from pathlib import Path
from mixseek.orchestrator.models import OrchestratorTask

def test_orchestrator_task_creation():
    """OrchestratorTaskä½œæˆãƒ†ã‚¹ãƒˆ"""
    task = OrchestratorTask(
        user_prompt="ãƒ†ã‚¹ãƒˆãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ",
        team_configs=[Path("team1.toml"), Path("team2.toml")],
        timeout_seconds=600,
    )
    assert task.user_prompt == "ãƒ†ã‚¹ãƒˆãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ"
    assert len(task.team_configs) == 2
    assert task.timeout_seconds == 600
    assert task.task_id is not None  # UUIDãŒè‡ªå‹•ç”Ÿæˆã•ã‚Œã‚‹
    assert task.created_at is not None

def test_orchestrator_task_validation():
    """OrchestratorTask ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ãƒ†ã‚¹ãƒˆ"""
    with pytest.raises(ValueError):
        OrchestratorTask(
            user_prompt="",  # ç©ºæ–‡å­—åˆ—
            team_configs=[],
            timeout_seconds=600,
        )

    with pytest.raises(ValueError):
        OrchestratorTask(
            user_prompt="ãƒ†ã‚¹ãƒˆ",
            team_configs=[],  # ç©ºãƒªã‚¹ãƒˆ
            timeout_seconds=600,
        )
```

**Dependencies**: ãªã—
**Expected**: ãƒ†ã‚¹ãƒˆå¤±æ•—ï¼ˆRedï¼‰

---

### T004 [P]: [US1] TeamStatus ãƒ¢ãƒ‡ãƒ«ã®ãƒ†ã‚¹ãƒˆä½œæˆ
**Story**: User Story 1
**File**: `tests/unit/orchestrator/test_models.py`
**Description**: TeamStatusãƒ¢ãƒ‡ãƒ«ã®ãƒ¦ãƒ‹ãƒƒãƒˆãƒ†ã‚¹ãƒˆã‚’ä½œæˆ

```python
def test_team_status_creation():
    """TeamStatusä½œæˆãƒ†ã‚¹ãƒˆ"""
    status = TeamStatus(
        team_id="team-001",
        team_name="Test Team",
    )
    assert status.team_id == "team-001"
    assert status.status == "pending"
    assert status.current_round == 0

def test_team_status_transitions():
    """TeamStatus ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹é·ç§»ãƒ†ã‚¹ãƒˆ"""
    status = TeamStatus(team_id="team-001", team_name="Test Team")
    status.status = "running"
    assert status.status == "running"

    status.status = "completed"
    assert status.status == "completed"
```

**Dependencies**: ãªã—
**Expected**: ãƒ†ã‚¹ãƒˆå¤±æ•—ï¼ˆRedï¼‰

---

### T005 [P]: [US2] RoundResult ãƒ¢ãƒ‡ãƒ«ã®ãƒ†ã‚¹ãƒˆä½œæˆ
**Story**: User Story 2 - ãƒ©ã‚¦ãƒ³ãƒ‰é€²è¡Œç®¡ç†ã¨çµ‚äº†åˆ¤å®š
**File**: `tests/unit/orchestrator/test_models.py`
**Description**: RoundResultãƒ¢ãƒ‡ãƒ«ã®ãƒ¦ãƒ‹ãƒƒãƒˆãƒ†ã‚¹ãƒˆã‚’ä½œæˆ

```python
from pydantic_ai import RunUsage

def test_round_result_creation():
    """RoundResultä½œæˆãƒ†ã‚¹ãƒˆ"""
    result = RoundResult(
        team_id="team-001",
        team_name="Test Team",
        round_number=1,
        submission_content="ãƒ†ã‚¹ãƒˆSubmission",
        evaluation_score=0.85,
        evaluation_feedback="è‰¯å¥½",
        usage=RunUsage(input_tokens=100, output_tokens=200, requests=1),
        execution_time_seconds=30.5,
    )
    assert result.team_id == "team-001"
    assert result.evaluation_score == 0.85
    assert result.execution_time_seconds == 30.5

def test_round_result_validation():
    """RoundResult ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ãƒ†ã‚¹ãƒˆ"""
    with pytest.raises(ValueError):
        RoundResult(
            team_id="team-001",
            team_name="Test Team",
            round_number=1,
            submission_content="ãƒ†ã‚¹ãƒˆ",
            evaluation_score=1.5,  # ç¯„å›²å¤–
            evaluation_feedback="",
            usage=RunUsage(),
            execution_time_seconds=30.0,
        )
```

**Dependencies**: ãªã—
**Expected**: ãƒ†ã‚¹ãƒˆå¤±æ•—ï¼ˆRedï¼‰

---

### T006 [P]: [US3] ExecutionSummary ãƒ¢ãƒ‡ãƒ«ã®ãƒ†ã‚¹ãƒˆä½œæˆ
**Story**: User Story 3 - å®Ÿè¡Œå…¨ä½“ã®å®Œäº†é›†ç´„ã¨çµ‚äº†é€šçŸ¥
**File**: `tests/unit/orchestrator/test_models.py`
**Description**: ExecutionSummaryãƒ¢ãƒ‡ãƒ«ã®ãƒ¦ãƒ‹ãƒƒãƒˆãƒ†ã‚¹ãƒˆã‚’ä½œæˆ

```python
def test_execution_summary_creation():
    """ExecutionSummaryä½œæˆãƒ†ã‚¹ãƒˆ"""
    result1 = RoundResult(...)  # ç•¥
    result2 = RoundResult(...)  # ç•¥

    summary = ExecutionSummary(
        task_id="task-123",
        user_prompt="ãƒ†ã‚¹ãƒˆãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ",
        team_results=[result1, result2],
        best_team_id="team-001",
        best_score=0.92,
        total_execution_time_seconds=45.3,
    )

    assert summary.total_teams == 2
    assert summary.completed_teams == 2
    assert summary.failed_teams == 0
```

**Dependencies**: T005å®Œäº†
**Expected**: ãƒ†ã‚¹ãƒˆå¤±æ•—ï¼ˆRedï¼‰

---

### T007 [P]: [US1] OrchestratorConfig ãƒ¢ãƒ‡ãƒ«ã®ãƒ†ã‚¹ãƒˆä½œæˆ
**Story**: User Story 1
**File**: `tests/unit/orchestrator/test_models.py`
**Description**: OrchestratorConfigãƒ¢ãƒ‡ãƒ«ã®ãƒ¦ãƒ‹ãƒƒãƒˆãƒ†ã‚¹ãƒˆã‚’ä½œæˆ

```python
def test_orchestrator_config_creation():
    """OrchestratorConfigä½œæˆãƒ†ã‚¹ãƒˆ"""
    config = OrchestratorConfig(
        timeout_per_team_seconds=600,
        teams=[
            TeamReference(config=Path("team1.toml")),
            TeamReference(config=Path("team2.toml")),
        ],
    )
    assert config.timeout_per_team_seconds == 600
    assert len(config.teams) == 2

def test_orchestrator_config_validation():
    """OrchestratorConfig ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ãƒ†ã‚¹ãƒˆ"""
    with pytest.raises(ValueError):
        OrchestratorConfig(
            timeout_per_team_seconds=-1,  # è² ã®å€¤
            teams=[],
        )
```

**Dependencies**: ãªã—
**Expected**: ãƒ†ã‚¹ãƒˆå¤±æ•—ï¼ˆRedï¼‰

---

### T008: [Foundational] ãƒ‡ãƒ¼ã‚¿ãƒ¢ãƒ‡ãƒ«å®Ÿè£…
**File**: `src/mixseek/orchestrator/models.py`
**Description**: å…¨ãƒ‡ãƒ¼ã‚¿ãƒ¢ãƒ‡ãƒ«ã‚’å®Ÿè£…ï¼ˆTDD: Green phaseï¼‰

```python
"""Orchestrator data models"""

from datetime import UTC, datetime
from pathlib import Path
from typing import Literal
from uuid import uuid4

from pydantic import BaseModel, Field, computed_field, field_validator
from pydantic_ai import RunUsage


class OrchestratorTask(BaseModel):
    """ãƒ¦ãƒ¼ã‚¶ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‹ã‚‰ç”Ÿæˆã•ã‚Œã‚‹ã‚¿ã‚¹ã‚¯å®šç¾©"""

    task_id: str = Field(default_factory=lambda: str(uuid4()), description="ã‚¿ã‚¹ã‚¯ä¸€æ„è­˜åˆ¥å­")
    user_prompt: str = Field(description="ãƒ¦ãƒ¼ã‚¶ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ")
    team_configs: list[Path] = Field(description="ãƒãƒ¼ãƒ è¨­å®šTOMLãƒ‘ã‚¹ãƒªã‚¹ãƒˆ")
    created_at: datetime = Field(default_factory=lambda: datetime.now(UTC), description="ä½œæˆæ—¥æ™‚")
    timeout_seconds: int = Field(gt=0, description="ãƒãƒ¼ãƒ å˜ä½ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆï¼ˆç§’ï¼‰")

    @field_validator("user_prompt")
    @classmethod
    def validate_user_prompt(cls, v: str) -> str:
        if not v or not v.strip():
            raise ValueError("user_prompt cannot be empty")
        return v

    @field_validator("team_configs")
    @classmethod
    def validate_team_configs(cls, v: list[Path]) -> list[Path]:
        if not v:
            raise ValueError("team_configs must have at least one config")
        return v


class TeamStatus(BaseModel):
    """ç‰¹å®šãƒãƒ¼ãƒ ã®å®Ÿè¡ŒçŠ¶æ…‹"""

    team_id: str = Field(description="ãƒãƒ¼ãƒ è­˜åˆ¥å­")
    team_name: str = Field(description="ãƒãƒ¼ãƒ å")
    status: Literal["pending", "running", "completed", "failed", "timeout"] = Field(
        default="pending",
        description="å®Ÿè¡Œã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹",
    )
    current_round: int = Field(default=0, ge=0, description="ç¾åœ¨ã®ãƒ©ã‚¦ãƒ³ãƒ‰ç•ªå·")
    started_at: datetime | None = Field(default=None, description="å®Ÿè¡Œé–‹å§‹æ—¥æ™‚")
    completed_at: datetime | None = Field(default=None, description="å®Ÿè¡Œå®Œäº†æ—¥æ™‚")
    error_message: str | None = Field(default=None, description="ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸")


class RoundResult(BaseModel):
    """1ãƒ©ã‚¦ãƒ³ãƒ‰ã®å®Ÿè¡Œçµæœ"""

    team_id: str = Field(description="ãƒãƒ¼ãƒ è­˜åˆ¥å­")
    team_name: str = Field(description="ãƒãƒ¼ãƒ å")
    round_number: int = Field(ge=1, description="ãƒ©ã‚¦ãƒ³ãƒ‰ç•ªå·")
    submission_content: str = Field(description="Submissionãƒ†ã‚­ã‚¹ãƒˆ")
    evaluation_score: float = Field(ge=0.0, le=1.0, description="è©•ä¾¡ã‚¹ã‚³ã‚¢")
    evaluation_feedback: str = Field(description="è©•ä¾¡ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯")
    usage: RunUsage = Field(description="ãƒªã‚½ãƒ¼ã‚¹ä½¿ç”¨é‡")
    execution_time_seconds: float = Field(gt=0, description="å®Ÿè¡Œæ™‚é–“ï¼ˆç§’ï¼‰")
    completed_at: datetime = Field(
        default_factory=lambda: datetime.now(UTC),
        description="å®Œäº†æ—¥æ™‚",
    )


class ExecutionSummary(BaseModel):
    """å…¨ãƒãƒ¼ãƒ ã®å®Œäº†å¾Œã«ç”Ÿæˆã•ã‚Œã‚‹æœ€çµ‚é›†ç´„æƒ…å ±"""

    task_id: str = Field(description="ã‚¿ã‚¹ã‚¯è­˜åˆ¥å­")
    user_prompt: str = Field(description="ãƒ¦ãƒ¼ã‚¶ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ")
    team_results: list[RoundResult] = Field(default_factory=list, description="ãƒãƒ¼ãƒ çµæœãƒªã‚¹ãƒˆ")
    best_team_id: str | None = Field(default=None, description="æœ€é«˜ã‚¹ã‚³ã‚¢ãƒãƒ¼ãƒ ID")
    best_score: float | None = Field(default=None, ge=0.0, le=1.0, description="æœ€é«˜è©•ä¾¡ã‚¹ã‚³ã‚¢")
    total_execution_time_seconds: float = Field(gt=0, description="ç·å®Ÿè¡Œæ™‚é–“ï¼ˆç§’ï¼‰")
    created_at: datetime = Field(
        default_factory=lambda: datetime.now(UTC),
        description="ã‚µãƒãƒªãƒ¼ä½œæˆæ—¥æ™‚",
    )

    @computed_field  # type: ignore[prop-decorator]
    @property
    def total_teams(self) -> int:
        """ç·ãƒãƒ¼ãƒ æ•°"""
        return len(self.team_results)

    @computed_field  # type: ignore[prop-decorator]
    @property
    def completed_teams(self) -> int:
        """å®Œäº†ãƒãƒ¼ãƒ æ•°"""
        return sum(1 for r in self.team_results if r.evaluation_score is not None)

    @computed_field  # type: ignore[prop-decorator]
    @property
    def failed_teams(self) -> int:
        """å¤±æ•—ãƒãƒ¼ãƒ æ•°"""
        return self.total_teams - self.completed_teams


class TeamReference(BaseModel):
    """ãƒãƒ¼ãƒ å‚ç…§"""

    config: Path = Field(description="ãƒãƒ¼ãƒ è¨­å®šTOMLãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹")


class OrchestratorConfig(BaseModel):
    """ã‚ªãƒ¼ã‚±ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¿è¨­å®š"""

    timeout_per_team_seconds: int = Field(gt=0, description="ãƒãƒ¼ãƒ å˜ä½ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆï¼ˆç§’ï¼‰")
    teams: list[TeamReference] = Field(min_length=1, description="ãƒãƒ¼ãƒ å‚ç…§ãƒªã‚¹ãƒˆ")
```

**Dependencies**: T003-T007å®Œäº†
**Verification**: `pytest tests/unit/orchestrator/test_models.py` ãŒå…¨ã¦ãƒ‘ã‚¹ï¼ˆGreenï¼‰
**Checkpoint**: âœ… Foundational Modelså®Œæˆ

---

## Phase 3: User Story 1 - ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆå—ä¿¡ã¨ãƒãƒ«ãƒãƒãƒ¼ãƒ èµ·å‹• (P1)

**Goal**: ãƒ¦ãƒ¼ã‚¶ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’å—ã‘å–ã‚Šã€è¤‡æ•°ãƒãƒ¼ãƒ ã®ãƒ©ã‚¦ãƒ³ãƒ‰ã‚³ãƒ³ãƒˆãƒ­ãƒ¼ãƒ©ã‚’èµ·å‹•ã—ã¦DuckDBã«è¨˜éŒ²

### T009: [US1] RoundController ãƒ†ã‚¹ãƒˆä½œæˆï¼ˆãƒ¢ãƒƒã‚¯ç‰ˆï¼‰
**Story**: User Story 1
**File**: `tests/unit/orchestrator/test_round_controller.py`
**Description**: RoundControllerã®åˆæœŸåŒ–ã¨ãƒãƒ¼ãƒ æƒ…å ±å–å¾—ã®ãƒ†ã‚¹ãƒˆã‚’ä½œæˆ

```python
import pytest
from pathlib import Path
from mixseek.orchestrator import RoundController

def test_round_controller_initialization():
    """RoundControlleråˆæœŸåŒ–ãƒ†ã‚¹ãƒˆ"""
    controller = RoundController(
        team_config_path=Path("tests/fixtures/team1.toml"),
        workspace=Path("workspace"),
        round_number=1,
    )
    assert controller.get_team_id() is not None
    assert controller.get_team_name() is not None

def test_round_controller_invalid_config():
    """RoundController ä¸æ­£ãªè¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ãƒ†ã‚¹ãƒˆ"""
    with pytest.raises(FileNotFoundError):
        RoundController(
            team_config_path=Path("nonexistent.toml"),
            workspace=Path("workspace"),
        )
```

**Dependencies**: T008å®Œäº†
**Expected**: ãƒ†ã‚¹ãƒˆå¤±æ•—ï¼ˆRedï¼‰

---

### T010: [US1] RoundController åŸºæœ¬å®Ÿè£…
**Story**: User Story 1
**File**: `src/mixseek/orchestrator/round_controller.py`
**Description**: RoundControllerã®åˆæœŸåŒ–ã¨åŸºæœ¬ãƒ¡ã‚½ãƒƒãƒ‰ã‚’å®Ÿè£…

```python
"""Round Controller - å˜ä¸€ãƒãƒ¼ãƒ ã®1ãƒ©ã‚¦ãƒ³ãƒ‰å®Ÿè¡Œç®¡ç†"""

import asyncio
import time
from pathlib import Path

from mixseek.agents.leader.config import TeamConfig, load_team_config
from mixseek.orchestrator.models import RoundResult


class RoundController:
    """å˜ä¸€ãƒãƒ¼ãƒ ã®1ãƒ©ã‚¦ãƒ³ãƒ‰å®Ÿè¡Œã‚’ç®¡ç†"""

    def __init__(
        self,
        team_config_path: Path,
        workspace: Path,
        round_number: int = 1,
    ) -> None:
        """RoundControllerã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ä½œæˆ

        Args:
            team_config_path: ãƒãƒ¼ãƒ è¨­å®šTOMLãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
            workspace: ãƒ¯ãƒ¼ã‚¯ã‚¹ãƒšãƒ¼ã‚¹ãƒ‘ã‚¹
            round_number: ãƒ©ã‚¦ãƒ³ãƒ‰ç•ªå·ï¼ˆåˆæœŸå®Ÿè£…ã§ã¯å¸¸ã«1ï¼‰

        Raises:
            FileNotFoundError: team_config_pathãŒå­˜åœ¨ã—ãªã„å ´åˆ
            ValidationError: ãƒãƒ¼ãƒ è¨­å®šãŒä¸æ­£ãªå ´åˆ
        """
        self.team_config = load_team_config(team_config_path, workspace)
        self.workspace = workspace
        self.round_number = round_number

    def get_team_id(self) -> str:
        """ãƒãƒ¼ãƒ è­˜åˆ¥å­ã‚’å–å¾—"""
        return self.team_config.team_id

    def get_team_name(self) -> str:
        """ãƒãƒ¼ãƒ åã‚’å–å¾—"""
        return self.team_config.team_name

    async def run_round(
        self,
        user_prompt: str,
        timeout_seconds: int,
    ) -> RoundResult:
        """1ãƒ©ã‚¦ãƒ³ãƒ‰ã‚’å®Ÿè¡Œã—ã€çµæœã‚’è¿”ã™ï¼ˆå¾Œç¶šã‚¿ã‚¹ã‚¯ã§å®Ÿè£…ï¼‰"""
        raise NotImplementedError("T014ã§å®Ÿè£…")
```

**Dependencies**: T009å®Œäº†
**Verification**: `pytest tests/unit/orchestrator/test_round_controller.py` ãŒãƒ‘ã‚¹
**Checkpoint**: RoundControlleråŸºæœ¬å®Ÿè£…å®Œäº†

---

### T011: [US1] Orchestrator ãƒ†ã‚¹ãƒˆä½œæˆï¼ˆã‚¿ã‚¹ã‚¯ç”Ÿæˆï¼‰
**Story**: User Story 1
**File**: `tests/unit/orchestrator/test_orchestrator.py`
**Description**: Orchestratorã®ã‚¿ã‚¹ã‚¯ç”Ÿæˆãƒ†ã‚¹ãƒˆã‚’ä½œæˆ

```python
import pytest
from pathlib import Path
from unittest.mock import AsyncMock, Mock, patch
from mixseek.orchestrator import Orchestrator, OrchestratorConfig, TeamReference

@pytest.fixture
def orchestrator_config():
    """ãƒ†ã‚¹ãƒˆç”¨ã‚ªãƒ¼ã‚±ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¿è¨­å®š"""
    return OrchestratorConfig(
        timeout_per_team_seconds=600,
        teams=[
            TeamReference(config=Path("tests/fixtures/team1.toml")),
            TeamReference(config=Path("tests/fixtures/team2.toml")),
        ],
    )

def test_orchestrator_initialization(orchestrator_config):
    """OrchestratoråˆæœŸåŒ–ãƒ†ã‚¹ãƒˆ"""
    orchestrator = Orchestrator(config=orchestrator_config, workspace=Path("workspace"))
    assert orchestrator.config == orchestrator_config

@pytest.mark.asyncio
async def test_orchestrator_execute_task_creation(orchestrator_config):
    """Orchestrator ã‚¿ã‚¹ã‚¯ç”Ÿæˆãƒ†ã‚¹ãƒˆ"""
    orchestrator = Orchestrator(config=orchestrator_config, workspace=Path("workspace"))

    # ãƒ¢ãƒƒã‚¯ã§RoundControllerã‚’å·®ã—æ›¿ãˆ
    with patch("mixseek.orchestrator.orchestrator.RoundController") as mock_rc:
        mock_rc.return_value.run_round = AsyncMock(return_value=Mock())

        summary = await orchestrator.execute(user_prompt="ãƒ†ã‚¹ãƒˆãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ")

        # ã‚¿ã‚¹ã‚¯ãŒç”Ÿæˆã•ã‚Œã¦ã„ã‚‹ã“ã¨ã‚’ç¢ºèª
        assert summary.user_prompt == "ãƒ†ã‚¹ãƒˆãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ"
```

**Dependencies**: T010å®Œäº†
**Expected**: ãƒ†ã‚¹ãƒˆå¤±æ•—ï¼ˆRedï¼‰

---

### T012: [US1] Orchestrator åŸºæœ¬å®Ÿè£…
**Story**: User Story 1
**File**: `src/mixseek/orchestrator/orchestrator.py`
**Description**: Orchestratorã®åˆæœŸåŒ–ã¨ã‚¿ã‚¹ã‚¯ç”Ÿæˆã‚’å®Ÿè£…

```python
"""Orchestrator - è¤‡æ•°ãƒãƒ¼ãƒ ã®ä¸¦åˆ—å®Ÿè¡Œç®¡ç†"""

import asyncio
import os
import time
from pathlib import Path

from mixseek.orchestrator.models import (
    ExecutionSummary,
    OrchestratorConfig,
    OrchestratorTask,
    RoundResult,
    TeamStatus,
)
from mixseek.orchestrator.round_controller import RoundController


def load_orchestrator_config(config_path: Path) -> OrchestratorConfig:
    """ã‚ªãƒ¼ã‚±ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¿è¨­å®šTOMLèª­ã¿è¾¼ã¿

    Args:
        config_path: è¨­å®šTOMLãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹

    Returns:
        OrchestratorConfig

    Raises:
        FileNotFoundError: ãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã—ãªã„å ´åˆ
    """
    import tomllib

    with open(config_path, "rb") as f:
        data = tomllib.load(f)

    return OrchestratorConfig(
        timeout_per_team_seconds=data["orchestrator"]["timeout_per_team_seconds"],
        teams=[
            {"config": Path(team["config"])}
            for team in data["orchestrator"]["teams"]
        ],
    )


class Orchestrator:
    """è¤‡æ•°ãƒãƒ¼ãƒ ã®ãƒ©ã‚¦ãƒ³ãƒ‰ã‚³ãƒ³ãƒˆãƒ­ãƒ¼ãƒ©ã‚’ç®¡ç†"""

    def __init__(
        self,
        config: OrchestratorConfig,
        workspace: Path | None = None,
    ) -> None:
        """Orchestratorã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ä½œæˆ

        Args:
            config: ã‚ªãƒ¼ã‚±ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¿è¨­å®š
            workspace: ãƒ¯ãƒ¼ã‚¯ã‚¹ãƒšãƒ¼ã‚¹ãƒ‘ã‚¹ï¼ˆNoneã®å ´åˆã¯MIXSEEK_WORKSPACEç’°å¢ƒå¤‰æ•°ã‹ã‚‰å–å¾—ï¼‰

        Raises:
            EnvironmentError: MIXSEEK_WORKSPACEæœªè¨­å®šæ™‚
        """
        self.config = config
        self.workspace = self._get_workspace(workspace)
        self.team_statuses: dict[str, TeamStatus] = {}

    def _get_workspace(self, workspace: Path | None) -> Path:
        """ãƒ¯ãƒ¼ã‚¯ã‚¹ãƒšãƒ¼ã‚¹ãƒ‘ã‚¹å–å¾—"""
        if workspace is not None:
            return workspace

        if "MIXSEEK_WORKSPACE" not in os.environ:
            raise OSError(
                "MIXSEEK_WORKSPACE environment variable is not set.\n"
                "Please set it: export MIXSEEK_WORKSPACE=/path/to/workspace"
            )

        return Path(os.environ["MIXSEEK_WORKSPACE"])

    async def execute(
        self,
        user_prompt: str,
        timeout_seconds: int | None = None,
    ) -> ExecutionSummary:
        """ãƒ¦ãƒ¼ã‚¶ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’å—ã‘å–ã‚Šã€è¤‡æ•°ãƒãƒ¼ãƒ ã‚’ä¸¦åˆ—å®Ÿè¡Œï¼ˆå¾Œç¶šã‚¿ã‚¹ã‚¯ã§å®Ÿè£…ï¼‰"""
        raise NotImplementedError("T016ã§å®Ÿè£…")

    async def get_team_status(self, team_id: str) -> TeamStatus:
        """ç‰¹å®šãƒãƒ¼ãƒ ã®ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹å–å¾—"""
        if team_id not in self.team_statuses:
            raise KeyError(f"Team not found: {team_id}")
        return self.team_statuses[team_id]

    async def get_all_team_statuses(self) -> list[TeamStatus]:
        """å…¨ãƒãƒ¼ãƒ ã®ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹å–å¾—"""
        return list(self.team_statuses.values())
```

**Dependencies**: T011å®Œäº†
**Verification**: `pytest tests/unit/orchestrator/test_orchestrator.py` ãŒãƒ‘ã‚¹
**Checkpoint**: OrchestratoråŸºæœ¬å®Ÿè£…å®Œäº†

---

### T013: [US1] æ—¢å­˜Evaluatorã®çµ±åˆãƒ†ã‚¹ãƒˆä½œæˆ
**Story**: User Story 1
**File**: `tests/unit/orchestrator/test_evaluator_integration.py`
**Description**: æ—¢å­˜Evaluatorï¼ˆsrc/mixseek/evaluator/ï¼‰ãŒorchestratorã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã§æ­£ã—ãå‹•ä½œã™ã‚‹ã“ã¨ã‚’ç¢ºèª

**è¨­è¨ˆåˆ¤æ–­**:
- SimpleEvaluatorã®ä»£ã‚ã‚Šã«æ—¢å­˜å®Ÿè£…ï¼ˆsrc/mixseek/evaluator/Evaluatorï¼‰ã‚’ä½¿ç”¨
- DRYåŸå‰‡éµå®ˆï¼ˆArticle 10ï¼‰: è©•ä¾¡ãƒ­ã‚¸ãƒƒã‚¯ã®é‡è¤‡ã‚’å›é¿
- ä¸€è²«æ€§ä¿è¨¼: ã™ã¹ã¦ã®ãƒãƒ¼ãƒ ã§åŒã˜è©•ä¾¡åŸºæº–ã‚’é©ç”¨

```python
"""æ—¢å­˜Evaluatorã®çµ±åˆãƒ†ã‚¹ãƒˆ"""

from pathlib import Path

import pytest

from mixseek.evaluator import Evaluator, EvaluationRequest, EvaluationResult


def test_evaluator_basic_usage(tmp_path: Path) -> None:
    """æ—¢å­˜Evaluatorã®åŸºæœ¬çš„ãªä½¿ç”¨æ–¹æ³•ã‚’ãƒ†ã‚¹ãƒˆ"""
    evaluator = Evaluator(workspace_path=tmp_path)

    request = EvaluationRequest(
        user_query="Pythonã¨ã¯ä½•ã§ã™ã‹ï¼Ÿ",
        submission="Pythonã¯é«˜æ°´æº–ãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°è¨€èªã§ã™ã€‚",
        team_id="test-team-001",
    )

    result: EvaluationResult = evaluator.evaluate(request)

    # EvaluationResultã®æ¤œè¨¼
    assert isinstance(result, EvaluationResult)
    assert 0.0 <= result.overall_score <= 100.0  # 0-100ã‚¹ã‚±ãƒ¼ãƒ«
    assert len(result.metrics) >= 1

    # MetricScoreã®æ¤œè¨¼
    for metric in result.metrics:
        assert 0.0 <= metric.score <= 100.0
        assert isinstance(metric.metric_name, str)
        assert isinstance(metric.evaluator_comment, str)


def test_evaluator_result_structure() -> None:
    """EvaluationResultã®æ§‹é€ ãŒæœŸå¾…é€šã‚Šã§ã‚ã‚‹ã“ã¨ã‚’ç¢ºèª"""
    evaluator = Evaluator()

    request = EvaluationRequest(
        user_query="Pythonã®åˆ©ç‚¹ã¯ä½•ã§ã™ã‹ï¼Ÿ",
        submission="Pythonã¯èª­ã¿ã‚„ã™ãã€è±Šå¯Œãªãƒ©ã‚¤ãƒ–ãƒ©ãƒªãŒã‚ã‚Šã€ã‚³ãƒŸãƒ¥ãƒ‹ãƒ†ã‚£ãŒæ´»ç™ºã§ã™ã€‚",
        team_id="team-alpha-001",
    )

    result = evaluator.evaluate(request)

    # EvaluationResultæ§‹é€ ã®æ¤œè¨¼
    assert hasattr(result, "metrics")
    assert hasattr(result, "overall_score")
    assert isinstance(result.metrics, list)
    assert isinstance(result.overall_score, float)
```

**Dependencies**: T012å®Œäº†
**Verification**: `pytest tests/unit/orchestrator/test_evaluator_integration.py` ãŒãƒ‘ã‚¹
**Checkpoint**: æ—¢å­˜Evaluatorçµ±åˆãƒ†ã‚¹ãƒˆå®Œäº†

---

### T014: [US1] RoundController.run_round() ãƒ†ã‚¹ãƒˆä½œæˆ
**Story**: User Story 1
**File**: `tests/unit/orchestrator/test_round_controller.py`
**Description**: RoundController.run_round()ã®ãƒ†ã‚¹ãƒˆã‚’ä½œæˆï¼ˆæ—¢å­˜Evaluatorã‚’ä½¿ç”¨ï¼‰

```python
from mixseek.evaluator import EvaluationResult
from mixseek.models.evaluation_result import MetricScore

@pytest.mark.asyncio
@patch("mixseek.orchestrator.round_controller.create_leader_agent")
@patch("mixseek.orchestrator.round_controller.AggregationStore")
@patch("mixseek.orchestrator.round_controller.Evaluator")
async def test_run_round_with_evaluator(
    mock_evaluator_class: MagicMock,
    mock_store_class: MagicMock,
    mock_create_leader: MagicMock,
    tmp_path: Path,
) -> None:
    """run_round()ãŒæ—¢å­˜Evaluatorã‚’ä½¿ç”¨ã—ã¦æ­£ã—ãå‹•ä½œã™ã‚‹ã“ã¨ã‚’ãƒ†ã‚¹ãƒˆ"""

    # Leader Agentã®ãƒ¢ãƒƒã‚¯
    mock_agent = AsyncMock()
    mock_result = MagicMock()
    mock_result.output = "ãƒ†ã‚¹ãƒˆSubmission"
    mock_result.all_messages.return_value = []
    mock_result.usage.return_value = MagicMock(input_tokens=100, output_tokens=50, requests=1)
    mock_agent.run.return_value = mock_result
    mock_create_leader.return_value = mock_agent

    # AggregationStoreã®ãƒ¢ãƒƒã‚¯
    mock_store = AsyncMock()
    mock_store_class.return_value = mock_store

    # Evaluatorã®ãƒ¢ãƒƒã‚¯ï¼ˆæ—¢å­˜Evaluatorï¼‰
    mock_evaluator = MagicMock()
    mock_evaluation_result = EvaluationResult(
        metrics=[
            MetricScore(metric_name="ClarityCoherence", score=85.5, evaluator_comment="æ˜ç­ã§ã™"),
            MetricScore(metric_name="Relevance", score=90.0, evaluator_comment="é–¢é€£æ€§ãŒé«˜ã„"),
        ],
        overall_score=87.75,  # 0-100ã‚¹ã‚±ãƒ¼ãƒ«
    )
    mock_evaluator.evaluate.return_value = mock_evaluation_result
    mock_evaluator_class.return_value = mock_evaluator

    # RoundControllerã®å®Ÿè¡Œ
    controller = RoundController(
        team_config_path=Path("tests/fixtures/team1.toml"),
        workspace=tmp_path,
        round_number=1,
    )

    result = await controller.run_round(
        user_prompt="ãƒ†ã‚¹ãƒˆãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ",
        timeout_seconds=60,
    )

    # çµæœã®æ¤œè¨¼ï¼ˆ0-100ã‚¹ã‚±ãƒ¼ãƒ«ï¼‰
    assert result.evaluation_score == 87.75
    assert "æ˜ç­ã§ã™" in result.evaluation_feedback
```

**Dependencies**: T013å®Œäº†
**Verification**: `pytest tests/unit/orchestrator/test_round_controller.py::test_run_round_with_evaluator` ãŒãƒ‘ã‚¹
**Expected**: ãƒ†ã‚¹ãƒˆå¤±æ•—ï¼ˆRedï¼‰

---

### T015: [US1] RoundController.run_round() å®Ÿè£…
**Story**: User Story 1
**File**: `src/mixseek/orchestrator/round_controller.py`
**Description**: RoundController.run_round()ã‚’å®Ÿè£…ï¼ˆLeader Agentå®Ÿè¡Œã€Evaluatorå®Ÿè¡Œã€DuckDBè¨˜éŒ²ï¼‰

```python
async def run_round(
    self,
    user_prompt: str,
    timeout_seconds: int,
) -> RoundResult:
    """1ãƒ©ã‚¦ãƒ³ãƒ‰ã‚’å®Ÿè¡Œã—ã€çµæœã‚’è¿”ã™"""
    start_time = time.time()

    # 1. Leader Agentå®Ÿè¡Œ
    from mixseek.agents.leader.agent import create_leader_agent
    from mixseek.agents.leader.dependencies import TeamDependencies

    # Member Agentãƒãƒƒãƒ—æº–å‚™ï¼ˆç°¡æ˜“å®Ÿè£…ï¼‰
    member_agents = {}  # TODO: Member Agentå®Ÿè£…å¾Œã«è¿½åŠ 

    leader_agent = create_leader_agent(self.team_config, member_agents)
    deps = TeamDependencies(
        team_id=self.team_config.team_id,
        team_name=self.team_config.team_name,
        workspace=self.workspace,
        round_number=self.round_number,
    )

    result = await leader_agent.run(user_prompt, deps=deps)
    submission_content = result.data
    message_history = result.all_messages()
    usage = result.usage()

    # 2. DuckDBè¨˜éŒ²ï¼ˆround_historyï¼‰
    from mixseek.storage.aggregation_store import AggregationStore
    from mixseek.agents.leader.models import MemberSubmissionsRecord

    store = AggregationStore(db_path=self.workspace / "mixseek.db")

    # Member Agentå¿œç­”è¨˜éŒ²ï¼ˆç°¡æ˜“ç‰ˆï¼‰
    member_record = MemberSubmissionsRecord(
        team_id=self.team_config.team_id,
        team_name=self.team_config.team_name,
        round_number=self.round_number,
        submissions=[],
    )

    await store.save_aggregation(member_record, message_history)

    # 3. Evaluatorå®Ÿè¡Œï¼ˆæ—¢å­˜Evaluatorã‚’ä½¿ç”¨ï¼‰
    from mixseek.evaluator import Evaluator, EvaluationRequest

    evaluator = Evaluator(workspace_path=self.workspace)
    request = EvaluationRequest(
        user_query=user_prompt,
        submission=submission_content,
        team_id=self.team_config.team_id,
    )

    # åŒæœŸãƒ¡ã‚½ãƒƒãƒ‰ã‚’éåŒæœŸã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã§å®Ÿè¡Œ
    result = await asyncio.to_thread(evaluator.evaluate, request)

    # 0-100ã‚¹ã‚±ãƒ¼ãƒ«ã®ã‚¹ã‚³ã‚¢ã‚’å–å¾—
    evaluation_score = result.overall_score

    # å„ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã®ã‚³ãƒ¡ãƒ³ãƒˆã‚’çµ±åˆã—ã¦ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã‚’ä½œæˆ
    evaluation_feedback = "\n".join(
        [f"{metric.metric_name} ({metric.score:.2f}): {metric.evaluator_comment}" for metric in result.metrics]
    )

    # 4. DuckDBè¨˜éŒ²ï¼ˆleader_boardï¼‰
    await store.save_to_leader_board(
        team_id=self.team_config.team_id,
        team_name=self.team_config.team_name,
        round_number=self.round_number,
        evaluation_score=evaluation_score,
        evaluation_feedback=evaluation_feedback,
        submission=submission_content,
        usage_info={
            "input_tokens": usage.input_tokens or 0,
            "output_tokens": usage.output_tokens or 0,
            "requests": usage.requests or 0,
        },
    )

    # 5. RoundResultç”Ÿæˆ
    execution_time = time.time() - start_time

    return RoundResult(
        team_id=self.team_config.team_id,
        team_name=self.team_config.team_name,
        round_number=self.round_number,
        submission_content=submission_content,
        evaluation_score=evaluation_score,
        evaluation_feedback=evaluation_feedback,
        usage=usage,
        execution_time_seconds=execution_time,
    )
```

**Dependencies**: T014å®Œäº†
**Verification**: `pytest tests/unit/orchestrator/test_round_controller.py` ãŒãƒ‘ã‚¹
**Checkpoint**: âœ… User Story 1 - RoundControllerå®Œæˆ

---

### T016: [US1] Orchestrator.execute() å®Ÿè£…
**Story**: User Story 1
**File**: `src/mixseek/orchestrator/orchestrator.py`
**Description**: Orchestrator.execute()ã‚’å®Ÿè£…ï¼ˆè¤‡æ•°ãƒãƒ¼ãƒ ä¸¦åˆ—å®Ÿè¡Œï¼‰

```python
async def execute(
    self,
    user_prompt: str,
    timeout_seconds: int | None = None,
) -> ExecutionSummary:
    """ãƒ¦ãƒ¼ã‚¶ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’å—ã‘å–ã‚Šã€è¤‡æ•°ãƒãƒ¼ãƒ ã‚’ä¸¦åˆ—å®Ÿè¡Œ"""
    if not user_prompt or not user_prompt.strip():
        raise ValueError("user_prompt cannot be empty")

    timeout = timeout_seconds or self.config.timeout_per_team_seconds

    # ã‚¿ã‚¹ã‚¯ç”Ÿæˆ
    task = OrchestratorTask(
        user_prompt=user_prompt,
        team_configs=[ref.config for ref in self.config.teams],
        timeout_seconds=timeout,
    )

    # TeamStatusåˆæœŸåŒ–
    for ref in self.config.teams:
        # ä¸€æ™‚çš„ã«ãƒãƒ¼ãƒ IDã‚’å–å¾—ï¼ˆè¨­å®šèª­ã¿è¾¼ã¿ï¼‰
        temp_config = load_team_config(ref.config, self.workspace)
        self.team_statuses[temp_config.team_id] = TeamStatus(
            team_id=temp_config.team_id,
            team_name=temp_config.team_name,
        )

    # RoundControllerä½œæˆ
    controllers = [
        RoundController(
            team_config_path=ref.config,
            workspace=self.workspace,
            round_number=1,
        )
        for ref in self.config.teams
    ]

    # ä¸¦åˆ—å®Ÿè¡Œ
    start_time = time.time()

    results = await asyncio.gather(
        *[
            self._run_team(controller, user_prompt, timeout)
            for controller in controllers
        ],
        return_exceptions=True,
    )

    execution_time = time.time() - start_time

    # çµæœåé›†
    team_results: list[RoundResult] = []
    for result in results:
        if isinstance(result, RoundResult):
            team_results.append(result)
            # TeamStatusæ›´æ–°
            self.team_statuses[result.team_id].status = "completed"
            self.team_statuses[result.team_id].completed_at = result.completed_at
        elif isinstance(result, Exception):
            # ã‚¨ãƒ©ãƒ¼å‡¦ç†ï¼ˆå¤±æ ¼ï¼‰
            pass

    # æœ€é«˜ã‚¹ã‚³ã‚¢ãƒãƒ¼ãƒ ç‰¹å®š
    best_team_id = None
    best_score = None

    if team_results:
        best_result = max(team_results, key=lambda r: r.evaluation_score)
        best_team_id = best_result.team_id
        best_score = best_result.evaluation_score

    # ExecutionSummaryç”Ÿæˆ
    return ExecutionSummary(
        task_id=task.task_id,
        user_prompt=user_prompt,
        team_results=team_results,
        best_team_id=best_team_id,
        best_score=best_score,
        total_execution_time_seconds=execution_time,
    )

async def _run_team(
    self,
    controller: RoundController,
    user_prompt: str,
    timeout_seconds: int,
) -> RoundResult:
    """ãƒãƒ¼ãƒ å˜ä½ã®å®Ÿè¡Œï¼ˆã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆä»˜ãï¼‰"""
    team_id = controller.get_team_id()

    # ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹æ›´æ–°: running
    self.team_statuses[team_id].status = "running"
    self.team_statuses[team_id].started_at = datetime.now(UTC)

    try:
        result = await asyncio.wait_for(
            controller.run_round(user_prompt, timeout_seconds),
            timeout=timeout_seconds,
        )
        return result
    except asyncio.TimeoutError:
        self.team_statuses[team_id].status = "timeout"
        self.team_statuses[team_id].error_message = f"Timeout after {timeout_seconds}s"
        raise
    except Exception as e:
        self.team_statuses[team_id].status = "failed"
        self.team_statuses[team_id].error_message = str(e)
        raise
```

**Dependencies**: T015å®Œäº†
**Verification**: `pytest tests/unit/orchestrator/test_orchestrator.py` ãŒãƒ‘ã‚¹
**Checkpoint**: âœ… User Story 1 å®Œæˆ

---

## Phase 4: User Story 2 - ãƒ©ã‚¦ãƒ³ãƒ‰é€²è¡Œç®¡ç†ã¨çµ‚äº†åˆ¤å®š (P2)

**Note**: åˆæœŸå®Ÿè£…ã§ã¯1ãƒ©ã‚¦ãƒ³ãƒ‰ã®ã¿ã®ãŸã‚ã€ã“ã®User Storyã¯ç°¡ç•¥åŒ–ã•ã‚Œã¾ã™ã€‚å°†æ¥çš„ã«è¤‡æ•°ãƒ©ã‚¦ãƒ³ãƒ‰å¯¾å¿œæ™‚ã«æ‹¡å¼µã—ã¾ã™ã€‚

### T017: [US2] ãƒ©ã‚¦ãƒ³ãƒ‰çµ‚äº†æ¡ä»¶åˆ¤å®šã®è¨­è¨ˆ
**Story**: User Story 2
**File**: `docs/future-enhancements.md`
**Description**: å°†æ¥ã®è¤‡æ•°ãƒ©ã‚¦ãƒ³ãƒ‰å¯¾å¿œã®ãŸã‚ã®è¨­è¨ˆãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’ä½œæˆ

```markdown
# Future Enhancements: Multiple Round Support

## Overview
åˆæœŸå®Ÿè£…ã¯1ãƒ©ã‚¦ãƒ³ãƒ‰ã®ã¿ã ãŒã€å°†æ¥çš„ã«è¤‡æ•°ãƒ©ã‚¦ãƒ³ãƒ‰å¯¾å¿œã‚’è¡Œã†éš›ã®è¨­è¨ˆæŒ‡é‡ã€‚

## RoundControlleræ‹¡å¼µ
- `should_continue_round()`: ãƒ©ã‚¦ãƒ³ãƒ‰ç¶™ç¶šåˆ¤å®š
- `load_previous_round()`: å‰ãƒ©ã‚¦ãƒ³ãƒ‰çµæœèª­ã¿è¾¼ã¿
- `update_round_state()`: ãƒ©ã‚¦ãƒ³ãƒ‰çŠ¶æ…‹æ›´æ–°

## Orchestratoræ‹¡å¼µ
- ãƒ©ã‚¦ãƒ³ãƒ‰é€²è¡ŒçŠ¶æ³ã®ç›£è¦–
- ãƒ©ã‚¦ãƒ³ãƒ‰é–“ã®ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯çµ±åˆ

## å®Ÿè£…æ™‚ã®è€ƒæ…®äº‹é …
- FR-003: ãƒ©ã‚¦ãƒ³ãƒ‰ã‚³ãƒ³ãƒˆãƒ­ãƒ¼ãƒ©ã¸ã®å§”è­²
- FR-004: çµ‚äº†æ¡ä»¶åˆ¤å®š
```

**Dependencies**: T016å®Œäº†
**Checkpoint**: âœ… User Story 2 è¨­è¨ˆå®Œäº†ï¼ˆå®Ÿè£…ã¯å°†æ¥ï¼‰

---

## Phase 5: User Story 3 - å®Ÿè¡Œå…¨ä½“ã®å®Œäº†é›†ç´„ã¨çµ‚äº†é€šçŸ¥ (P3)

### T018: [US3] CLI exec ã‚³ãƒãƒ³ãƒ‰ã®ãƒ†ã‚¹ãƒˆä½œæˆ
**Story**: User Story 3
**File**: `tests/integration/test_orchestrator_e2e.py`
**Description**: CLIã‚³ãƒãƒ³ãƒ‰ã®E2Eãƒ†ã‚¹ãƒˆã‚’ä½œæˆ

```python
import subprocess
import json
from pathlib import Path

def test_mixseek_exec_help():
    """mixseek exec --helpãƒ†ã‚¹ãƒˆ"""
    result = subprocess.run(
        ["mixseek", "exec", "--help"],
        capture_output=True,
        text=True,
    )
    assert result.returncode == 0
    assert "USER_PROMPT" in result.stdout

def test_mixseek_exec_json_output(tmp_path):
    """mixseek exec JSONå‡ºåŠ›ãƒ†ã‚¹ãƒˆ"""
    # ãƒ†ã‚¹ãƒˆç”¨è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«æº–å‚™
    # ...

    result = subprocess.run(
        ["mixseek", "exec", "ãƒ†ã‚¹ãƒˆãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ", "--output", "json"],
        capture_output=True,
        text=True,
        env={"MIXSEEK_WORKSPACE": str(tmp_path)},
    )

    if result.returncode == 0:
        output = json.loads(result.stdout)
        assert "task_id" in output
        assert "user_prompt" in output
```

**Dependencies**: T016å®Œäº†
**Expected**: ãƒ†ã‚¹ãƒˆå¤±æ•—ï¼ˆRedï¼‰

---

### T019: [US3] CLI exec ã‚³ãƒãƒ³ãƒ‰å®Ÿè£…
**Story**: User Story 3
**File**: `src/mixseek/cli/commands/exec.py`
**Description**: `mixseek exec`ã‚³ãƒãƒ³ãƒ‰ã‚’å®Ÿè£…

```python
"""mixseek exec ã‚³ãƒãƒ³ãƒ‰å®Ÿè£…"""

import asyncio
import json
import sys
from pathlib import Path

import typer

from mixseek.orchestrator import Orchestrator, load_orchestrator_config


def exec(
    user_prompt: str = typer.Argument(..., help="ãƒ¦ãƒ¼ã‚¶ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ"),
    config: Path = typer.Option(
        "workspace/configs/orchestrator.toml",
        "--config",
        help="ã‚ªãƒ¼ã‚±ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¿è¨­å®šTOMLãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹",
    ),
    timeout: int | None = typer.Option(
        None,
        "--timeout",
        help="ãƒãƒ¼ãƒ å˜ä½ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆï¼ˆç§’ï¼‰",
    ),
    workspace: Path | None = typer.Option(
        None,
        "--workspace",
        help="ãƒ¯ãƒ¼ã‚¯ã‚¹ãƒšãƒ¼ã‚¹ãƒ‘ã‚¹ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: $MIXSEEK_WORKSPACEï¼‰",
    ),
    output: str = typer.Option(
        "text",
        "--output",
        help="å‡ºåŠ›ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆï¼ˆtext/jsonï¼‰",
    ),
    verbose: bool = typer.Option(
        False,
        "--verbose",
        "-v",
        help="è©³ç´°ãƒ­ã‚°è¡¨ç¤º",
    ),
) -> None:
    """ãƒ¦ãƒ¼ã‚¶ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’è¤‡æ•°ãƒãƒ¼ãƒ ã§ä¸¦åˆ—å®Ÿè¡Œ

    Note: exec ã‚³ãƒãƒ³ãƒ‰ã§ã¯ãƒªãƒ¼ãƒ€ãƒ¼ãƒœãƒ¼ãƒ‰æ©Ÿèƒ½ã®ãŸã‚ã€å¸¸ã« DuckDB ã«ä¿å­˜ã•ã‚Œã¾ã™ã€‚
    """

    async def _execute() -> None:
        try:
            # è¨­å®šèª­ã¿è¾¼ã¿
            orchestrator_config = load_orchestrator_config(config)

            # Orchestratorä½œæˆ
            orchestrator = Orchestrator(
                config=orchestrator_config,
                workspace=workspace,
            )

            # å®Ÿè¡Œ
            if output == "text":
                typer.echo("ğŸš€ MixSeek Orchestrator")
                typer.echo("â”" * 60)
                typer.echo(f"\nğŸ“ Task: {user_prompt}\n")
                typer.echo(f"ğŸ”„ Running {len(orchestrator_config.teams)} teams in parallel...\n")

            summary = await orchestrator.execute(
                user_prompt=user_prompt,
                timeout_seconds=timeout,
            )

            # å‡ºåŠ›
            if output == "json":
                print(summary.model_dump_json(indent=2))
            else:
                _print_text_summary(summary)

        except Exception as e:
            typer.echo(f"Error: {e}", err=True)
            sys.exit(1)

    asyncio.run(_execute())


def _print_text_summary(summary) -> None:
    """ãƒ†ã‚­ã‚¹ãƒˆå½¢å¼ã§ã‚µãƒãƒªãƒ¼ã‚’è¡¨ç¤º"""
    # ãƒãƒ¼ãƒ çµæœè¡¨ç¤º
    for result in summary.team_results:
        typer.echo(f"âœ… Team {result.team_id}: {result.team_name} (completed in {result.execution_time_seconds:.1f}s)")
        typer.echo(f"   Score: {result.evaluation_score:.2f}")
        typer.echo(f"   Feedback: {result.evaluation_feedback}\n")

    # æœ€é«˜ã‚¹ã‚³ã‚¢ãƒãƒ¼ãƒ è¡¨ç¤º
    if summary.best_team_id:
        typer.echo("â”" * 60)
        typer.echo(f"ğŸ† Best Result (Team {summary.best_team_id}, Score: {summary.best_score:.2f})")
        typer.echo("â”" * 60)

        best_result = next(r for r in summary.team_results if r.team_id == summary.best_team_id)
        typer.echo(f"\n{best_result.submission_content}\n")

    # ã‚µãƒãƒªãƒ¼è¡¨ç¤º
    typer.echo("â”" * 60)
    typer.echo("ğŸ“Š Summary")
    typer.echo("â”" * 60)
    typer.echo(f"\nTotal Teams:      {summary.total_teams}")
    typer.echo(f"Completed Teams:  {summary.completed_teams}")
    typer.echo(f"Failed Teams:     {summary.failed_teams}")
    typer.echo(f"Execution Time:   {summary.total_execution_time_seconds:.1f}s")
    typer.echo(f"\nğŸ’¾ Results saved to DuckDB")
```

**Dependencies**: T018å®Œäº†

---

### T020: [US3] CLI main.pyã«execã‚³ãƒãƒ³ãƒ‰ç™»éŒ²
**Story**: User Story 3
**File**: `src/mixseek/cli/main.py`
**Description**: main.pyã«execã‚³ãƒãƒ³ãƒ‰ã‚’ç™»éŒ²

```python
# æ—¢å­˜ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
from mixseek.cli.commands import evaluate as evaluate_module
from mixseek.cli.commands import init as init_module
from mixseek.cli.commands import member as member_module
from mixseek.cli.commands import team as team_module
from mixseek.cli.commands import validate_models as validate_models_module
from mixseek.cli.commands import exec as exec_module  # è¿½åŠ 

# æ—¢å­˜ã®ã‚³ãƒãƒ³ãƒ‰ç™»éŒ²
app.command(name="init")(init_module.init)
app.command(name="member")(member_module.member)
app.command(name="team")(team_module.team)
app.command(name="validate-models")(validate_models_module.validate_models)
app.command(name="evaluate")(evaluate_module.evaluate)
app.command(name="exec")(exec_module.exec)  # è¿½åŠ 
```

**Dependencies**: T019å®Œäº†
**Verification**: `pytest tests/integration/test_orchestrator_e2e.py` ãŒãƒ‘ã‚¹
**Checkpoint**: âœ… User Story 3 å®Œæˆ

---

## Phase 6: Polish & Cross-Cutting Concerns

### T021 [P]: å‹ãƒã‚§ãƒƒã‚¯ï¼ˆmypyï¼‰
**File**: å…¨ãƒ•ã‚¡ã‚¤ãƒ«
**Description**: mypy strict modeã§å‹ãƒã‚§ãƒƒã‚¯ã‚’å®Ÿè¡Œã—ã€ã‚¨ãƒ©ãƒ¼ã‚’ä¿®æ­£

```bash
mypy src/mixseek/orchestrator/
```

**Dependencies**: T020å®Œäº†

---

### T022 [P]: ã‚³ãƒ¼ãƒ‰ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆï¼ˆruffï¼‰
**File**: å…¨ãƒ•ã‚¡ã‚¤ãƒ«
**Description**: ruffã§ã‚³ãƒ¼ãƒ‰ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã¨ãƒªãƒ³ãƒˆã‚’å®Ÿè¡Œ

```bash
ruff check --fix src/mixseek/orchestrator/
ruff format src/mixseek/orchestrator/
```

**Dependencies**: T020å®Œäº†

---

### T023: çµ±åˆãƒ†ã‚¹ãƒˆã®å®Ÿè¡Œ
**File**: å…¨ãƒ†ã‚¹ãƒˆ
**Description**: å…¨ãƒ†ã‚¹ãƒˆã‚’å®Ÿè¡Œã—ã¦å“è³ªã‚’ç¢ºèª

```bash
pytest tests/unit/orchestrator/
pytest tests/integration/test_orchestrator_e2e.py
```

**Dependencies**: T021, T022å®Œäº†
**Checkpoint**: âœ… å…¨ãƒ†ã‚¹ãƒˆãƒ‘ã‚¹

---

## Dependencies Graph

```
Phase 1: Setup
T001 â†’ T002 â†’ Phase 2

Phase 2: Foundational Models (blocking all user stories)
T003 [P]  â”
T004 [P]  â”œâ†’ T008 â†’ Phase 3
T005 [P]  â”œâ†’ (T006ä¾å­˜)
T006 [P]  â”‚
T007 [P]  â”˜

Phase 3: User Story 1 (P1)
T008 â†’ T009 â†’ T010 â†’ T011 â†’ T012 â†’ T013 â†’ T014 â†’ T015 â†’ T016

Phase 4: User Story 2 (P2) - Simplified
T016 â†’ T017

Phase 5: User Story 3 (P3)
T016 â†’ T018 â†’ T019 â†’ T020

Phase 6: Polish
T020 â†’ T021 [P]
T020 â†’ T022 [P]
T021, T022 â†’ T023
```

## Parallel Execution Opportunities

### Phase 2: Foundational Models
- **T003-T007**: 5ã¤ã®ãƒ¢ãƒ‡ãƒ«ãƒ†ã‚¹ãƒˆã‚’ä¸¦åˆ—ä½œæˆå¯èƒ½

### Phase 6: Polish
- **T021, T022**: å‹ãƒã‚§ãƒƒã‚¯ã¨ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã‚’ä¸¦åˆ—å®Ÿè¡Œå¯èƒ½

## Implementation Strategy

1. **MVP Scope**: User Story 1ã®ã¿ï¼ˆT001-T016ï¼‰
   - åŸºæœ¬çš„ãªã‚ªãƒ¼ã‚±ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¿æ©Ÿèƒ½
   - 1ãƒ©ã‚¦ãƒ³ãƒ‰ã®å®Ÿè¡Œ
   - DuckDBè¨˜éŒ²

2. **Full v1.0 Scope**: User Story 1-3ï¼ˆT001-T023ï¼‰
   - CLIã‚³ãƒãƒ³ãƒ‰å®Ÿè£…
   - å®Œäº†é€šçŸ¥ã¨çµæœè¡¨ç¤º
   - å…¨å“è³ªãƒã‚§ãƒƒã‚¯

3. **Future**: User Story 2ã®å®Œå…¨å®Ÿè£…ï¼ˆè¤‡æ•°ãƒ©ã‚¦ãƒ³ãƒ‰å¯¾å¿œï¼‰

## Task Summary

- **Total Tasks**: 23
- **User Story 1 (P1)**: 12 tasks (T003-T016)
- **User Story 2 (P2)**: 1 task (T017 - design only)
- **User Story 3 (P3)**: 3 tasks (T018-T020)
- **Setup**: 2 tasks (T001-T002)
- **Foundational**: 6 tasks (T003-T008)
- **Polish**: 3 tasks (T021-T023)
- **Parallel Opportunities**: 7 tasks (5 in Phase 2, 2 in Phase 6)

## Suggested MVP

**T001-T016** (16 tasks) ã§åŸºæœ¬çš„ãªã‚ªãƒ¼ã‚±ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¿æ©Ÿèƒ½ãŒå‹•ä½œã—ã¾ã™:
- ãƒ¦ãƒ¼ã‚¶ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆå—ä¿¡
- è¤‡æ•°ãƒãƒ¼ãƒ ä¸¦åˆ—å®Ÿè¡Œ
- DuckDBè¨˜éŒ²
- çµæœé›†ç´„

CLIã‚³ãƒãƒ³ãƒ‰ã‚’å«ã‚€å®Œå…¨ç‰ˆã¯ **T001-T023** (23 tasks) ã§ã™ã€‚
