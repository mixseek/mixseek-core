# 機能仕様書: AIエージェント出力評価器

**機能ブランチ**: `022-mixseek-core-evaluator`  
**作成日**: 2025-01-20  
**ステータス**: ドラフト  
**入力**: ユーザ説明: "AIエージェントが出力した結果を評価するためのEvaluatorコンポーネントの実装を行う"

## 明確化事項

### セッション 2025-01-21

- Q: LLM-as-a-Judge実装にローカルモデルかクラウドAPIか？ → A: クラウドベースLLM API（OpenAI、Anthropic等）を使用
- Q: 組み込み評価指標の範囲は？ → A: ClarityCoherence、Coverage、Relevanceの3つの組み込み指標のみ（注: その後の要件追加により、汎用評価指標(LLMPlain)が4つ目の組み込み指標として追加された）
- Q: LLM APIエラー時の対処法は？ → A: リトライ回数を定数として持ち、どれか一つでもリトライ後に失敗した評価指標があれば例外を発生させる
- Q: TOMLファイルでの指標重み設定形式は？ → A: [[metric]] name = "clarity_coherence", weight = 0.4 のような配列形式
- Q: 重みが合計1.0にならない場合の処理は？ → A: エラーを返し、ユーザーに正しい重み（合計1.0）を設定するよう促す

### セッション 2025-01-22

- Q: 評価結果の永続化方法（複数プロセスからの同時アクセスを考慮）は？ → A: インメモリのみで返却；呼び出し側が永続化の責任を持つ
- Q: AI出力が空または空白文字のみの場合の処理は？ → A: エラーを返し、評価を中断する（呼び出し側がエラーハンドリングを実装）
- Q: 極端に長い応答（10,000文字超）の処理は？ → A: 現時点では応答の長さをハンドリングせず、そのまま評価する
- Q: 複数の評価指標の実行方式は？ → A: 各評価指標を順次評価（シンプルで予測可能）

### セッション 2025-10-22

- Q: ユーザクエリが不正な形式または不明確な場合、システムはどう処理すべきですか？ → A: そのまま評価を実行する（不明確なクエリでもLLM-as-a-Judgeが評価可能。呼び出し側がクエリの妥当性を保証する責任を持つ）
- Q: LLM-as-a-Judgeサービスが完全に利用不可能な場合（全ての指標でリトライ後も失敗）、システムはどう処理すべきですか？ → A: 例外を発生させ、呼び出し側にエラーを通知する（評価不可能な状態を明示的に伝える）
- Q: 一部の評価指標のみがリトライ後も失敗した場合、システムはどう処理すべきですか？ → A: どれか一つでもリトライ後に失敗した評価指標があれば例外を発生させる（部分的な評価結果は返さず、全ての指標が成功することを要求する）
- Q: TOML設定に競合する、または不可能な重みの組み合わせが含まれる場合、システムはどう処理すべきですか？ → A: 設定ロード時に検証エラーを返す（負の重み、合計が1.0でない重みなど、無効な設定を事前に拒否）
- Q: Pydantic AIのDirect Model Request APIに切り替える際、LLM-as-a-Judge呼び出しにどのAPIバリアントを使用すべきですか？ → A: `model_request_sync`（同期・非ストリーミング）
- Q: Pydantic AIの実装時に参照すべきドキュメントは？ → A: specs/006-evaluator/assets/docs/pydantic-ai/ 配下のドキュメントを参照して正確な実装を行う
- Q: 各評価指標ごとにLLMプロバイダーを設定する際のスコープは？ → A: 各指標が独自のプロバイダーを指定でき、グローバルなデフォルト値にフォールバックする
- Q: TOML設定ファイルでLLMプロバイダーを指定する際の設定形式は？ → A: 各metric配列要素内にproviderフィールドを追加、グローバルdefault_providerも設定可能
- Q: プロバイダーを指定する際の値の形式は？ → A: 文字列識別子 + オプションのmodelフィールド（例：provider = "anthropic", model = "claude-3-5-sonnet-20241022"）
- Q: LLMプロバイダーの認証情報の管理方法は？ → A: 環境変数から取得（例：OPENAI_API_KEY、ANTHROPIC_API_KEY）
- Q: 特定の指標で指定されたLLMプロバイダーが利用不可能な場合の動作は？ → A: 即座にエラーを返し、評価を中断する（厳格、設定ミスを早期検知）
- Q: Evaluatorのデータモデルの配置場所は？ → A: 既存の `src/mixseek/models/` 配下に配置（既存構造との一貫性を保つ）
- Q: Evaluatorの実装コード（評価ロジック、LLM呼び出し等）の配置ディレクトリは？ → A: `src/mixseek/evaluator/` 配下に配置（機能を明確に分離）
- Q: 評価器のTOML設定ファイルの配置場所は？ → A: ワークスペースディレクトリ直下の `configs/evaluator.toml`（プロジェクトごとに独立した設定管理）

### セッション 2025-10-30

- Q: TOMLでメトリクスレベルの`model`、`system_instruction`、`temperature`、`max_tokens`、`max_retries`が指定されておらず、対応するフィールドが`[llm_default]`にも存在しない場合、システムはどう処理すべきですか？ → A: メトリクスレベルの設定が優先され、設定がない場合は`[llm_default]`にフォールバック、それもない場合はハードコードされたデフォルト値を使用
- Q: TOMLで評価指標をクラス名で識別する際、システムはどのようにクラスを特定・ロードすべきですか？ → A: クラス名のみを指定（例: `name = "ClarityCoherence"`）し、システムが自動的に`src/mixseek/evaluator/metrics/`ディレクトリから該当クラスを検索
- Q: 新しい評価指標`LLMPlain`について、`system_instruction`が指定されていない場合、システムはどう処理すべきですか？ → A: `LLMPlain`もデフォルトのsystem_instructionを持ち、未指定時はそれを使用（例: "Evaluate the quality of the response."）
- Q: 既存の3つの組み込み指標（ClarityCoherence、Coverage、Relevance）について、TOML設定で`system_instruction`を指定した場合、システムはどう処理すべきですか？ → A: 既存の3つの指標もsystem_instruction上書きをサポートし、指定された場合はデフォルトのプロンプトを置き換える
- Q: TOML設定ファイルに含まれるLLMパラメータ（model、system_instruction、temperature等）や評価指標の設定の妥当性を、システムはいつ検証すべきですか？ → A: 設定ロード時（Evaluator初期化時）に全ての設定項目を検証し、不正な設定があれば即座にエラーを返す

## ユーザシナリオとテスト *(必須)*

### ユーザストーリー1 - 組み込み指標による基本評価 (優先度: P1)

Evaluatorは、どの応答がユーザのユースケースに最も適しているかを判断するために、標準的な品質指標を使用してAIエージェントの出力を評価する必要があります。

**この優先度の理由**: AI出力の品質評価を可能にすることで、評価コンポーネントの基本的な目的である中核的価値を提供します。

**独立テスト**: サンプルのAI応答とユーザクエリを提供し、評価を実行して、応答品質を正確に反映する数値スコアを受け取ることで完全にテストできます。

**受け入れシナリオ**:

1. **前提** ユーザクエリとAIエージェントの応答がある時、**実行** Evaluatorがデフォルトの組み込み指標（ClarityCoherence、Coverage、Relevance）で評価を実行する、**結果** システムは各指標について0-100のスコアと統合スコアをPydanticモデル（EvaluationResult）で返す
2. **前提** 単一のクエリ-応答ペアがある時、**実行** 評価を実行する、**結果** 複数の評価指標（ClarityCoherence、Coverage、Relevance）が評価され、各指標のscoreとevaluator_commentを含むMetricScoreオブジェクトのリストと統合スコア（overall_score）を持つEvaluationResultオブジェクトが返却される

---

### ユーザストーリー2 - カスタム評価設定 (優先度: P2)

Evaluatorは、特定の品質要件とドメイン専門知識に合わせて評価基準と重みをカスタマイズする必要があります。

**この優先度の理由**: 異なるドメインやユースケースのカスタマイズを可能にし、評価器を柔軟で幅広く適用できるようにします。

**独立テスト**: カスタム重みを持つTOML設定ファイルを作成し、評価を実行して、統合スコアが指定された優先度を反映していることを検証することでテストできます。

**受け入れシナリオ**:

1. **前提** カスタム重み（例：Relevance: 0.5、ClarityCoherence: 0.3、Coverage: 0.2）を指定するTOML設定がある時、**実行** 評価を実行する、**結果** 統合スコアが重み付けされた優先度を反映する
2. **前提** 特定の指標を無効にする設定がある時、**実行** 評価を実行する、**結果** 有効な指標のみが最終スコアに寄与する
3. **前提** 無効な設定値がある時、**実行** システムが設定をロードする、**結果** 明確なエラーメッセージがユーザに設定修正を案内する
4. **前提** 各評価指標に異なるLLMモデル（例：ClarityCoherenceに`"anthropic:claude-4-5-sonnet"`、Relevanceに`"openai:gpt-5"`）を指定するTOML設定がある時、**実行** 評価を実行する、**結果** 各指標が指定されたモデルで評価され、正しいスコアを返す
5. **前提** グローバルなdefault_modelを設定し、一部の指標でのみ個別のモデルを指定する設定がある時、**実行** 評価を実行する、**結果** 個別指定のある指標は指定モデルを使用し、未指定の指標はdefault_modelを使用する
6. **前提** 指定されたLLMプロバイダーのAPIキーが環境変数に設定されていない時、**実行** 評価を実行する、**結果** 明確なエラーメッセージ（どのプロバイダーのAPIキーが必要か）を返し、評価を中断する
7. **前提** 組み込み指標（例：ClarityCoherence）に対してカスタムsystem_instructionを指定するTOML設定がある時、**実行** 評価を実行する、**結果** 指定されたsystem_instructionがデフォルトのプロンプトを置き換え、カスタマイズされた基準で評価が実行される

---

### ユーザストーリー3 - カスタム指標統合 (優先度: P3)

上級ユーザは、組み込みオプションではカバーされないドメイン固有の評価指標を追加する必要があります。

**この優先度の理由**: 特殊なユースケースの拡張性を提供しますが、基本機能には必須ではありません。

**独立テスト**: カスタム指標関数を実装し、評価パイプラインに統合して、統合スコアに正しく寄与することを検証することでテストできます。

**受け入れシナリオ**:

1. **前提** 必要なインターフェースに従うカスタム指標関数がある時、**実行** 評価器に登録する、**結果** 設定で使用可能になる
2. **前提** ドメイン固有の基準を分析するカスタム指標がある時、**実行** 評価を実行する、**結果** カスタム指標スコアが組み込み指標と並んで結果に含まれる

---

### エッジケース

- AI出力が空または空白文字のみの場合、システムはエラーを返し評価を中断する。呼び出し側がエラーハンドリングを実装する責任を持つ
- ユーザクエリが不正な形式または不明確な場合、システムはそのまま評価を実行する。LLM-as-a-Judgeが評価可能であり、呼び出し側がクエリの妥当性を保証する責任を持つ
- LLM-as-a-Judgeサービスが利用不可能またはエラーを返した場合、FR-010に従い設定可能なリトライを実行する。どれか一つでもリトライ後に失敗した評価指標があれば、例外を発生させ呼び出し側にエラーを通知する
- 極端に長い応答（10,000文字超）は、現時点では特別なハンドリングを行わず、そのまま評価する。LLM APIの制限により失敗した場合は既存のエラーハンドリング（FR-010）で対処する
- TOML設定に競合する、または不可能な重みの組み合わせ（負の重み、合計が1.0でない重みなど）が含まれる場合、FR-009に従い設定ロード時に検証エラーを返し、無効な設定を事前に拒否する
- 特定の指標で指定されたLLMプロバイダーが利用不可能（APIキー未設定、サービス障害等）な場合、FR-018に従い即座にエラーを返し評価を中断する。設定ミスを早期検知するため、デフォルトプロバイダーへのフォールバックは行わない
- TOML設定でクラス名が指定されているが、`src/mixseek/evaluator/metrics/`ディレクトリに該当クラスが存在しない場合、FR-020に従い設定ロード時にエラーを返し、利用可能な評価指標のリストをユーザーに提示する
- 将来的にLLM-as-a-Judge以外の評価指標（例：統計ベースの指標）が追加された場合、それらの指標に対してLLMパラメータ（model、system_instruction等）が指定されても無視される（エラーにはしない）

## 要件 *(必須)*

### 機能要件

- **FR-001**: システムは4つの組み込み指標（ClarityCoherence、Coverage、Relevance、LLMPlain）を使用してAIエージェントの出力を評価しなければならない。すべての組み込み指標はデフォルトのsystem_instructionを持ち、TOML設定でsystem_instructionを指定することでデフォルトのプロンプトを上書きできる
- **FR-002**: システムは各評価指標について0-100の範囲でスコアを返さなければならない
- **FR-003**: システムは `{workspace}/configs/evaluator.toml` ファイルで [[metrics]] name = "ClarityCoherence", weight = 0.4, model = "anthropic:claude-4-5-sonnet", system_instruction = "..." 形式での指標重み、LLMモデル、system_instruction上書き、その他のLLMパラメータについての設定をサポートしなければならない。system_instructionが指定された場合、デフォルトのプロンプトを完全に置き換える
- **FR-004**: システムは加重平均を使用して個別指標スコアを統合しなければならない
- **FR-005**: システムはクラウドベースLLM API（OpenAI、Anthropic等）を使用したLLM-as-a-Judge実装にPydantic AIのDirect Model Request API（`model_request_sync`）を使用しなければならない
- **FR-006**: システムは型安全性のため、すべての入力と出力をPydanticモデルで定義しなければならない
- **FR-007**: システムは`BaseMetric`を継承したPythonクラスとしてカスタム評価指標を追加できるようにしなければならない
- **FR-008**: システムは重みが指定されない場合、デフォルトとして均等重み付けを提供しなければならない
- **FR-009**: システムは設定ファイルをEvaluator初期化時に検証し、重みの合計が1.0でない場合、不正なLLMパラメータ（負のtemperature、不正なmodel形式等）、存在しない評価指標クラス名など、無効な設定に対して明確なエラーメッセージを提供しなければならない。設定検証は評価実行前に完了し、不正な設定があれば即座にエラーを返す
- **FR-010**: システムはtomlで設定可能なグローバルなリトライ回数でLLM APIの失敗を処理し、どれか一つでもリトライ後に失敗した評価指標があれば例外を発生させなければならない（全ての評価指標が成功することを要求する）
- **FR-011**: システムは評価結果をインメモリで返却し、永続化層を含まない（呼び出し側が永続化の責任を持つ）
- **FR-012**: システムは評価結果をPydanticモデル（EvaluationResult）で返却しなければならない。EvaluationResultは各指標のMetricScoreオブジェクトのリスト（metrics）と統合スコア（overall_score）をフィールドとして持つ
- **FR-013**: システムはAI出力が空または空白文字のみの場合、入力検証エラーを返し評価を中断しなければならない
- **FR-014**: システムは複数の評価指標を順次実行し、実装をシンプルで予測可能に保たなければならない
- **FR-015**: システムは各評価指標ごとにLLMモデルを個別に設定できなければならない。各metric要素内にmodelフィールドを持ち、`"provider:model-name"`形式（例：`"anthropic:claude-4-5-sonnet"`）で指定する
- **FR-016**: システムはグローバルなデフォルトLLMモデル（default_model）をTOML設定で指定可能にし、各指標でモデルが指定されていない場合はデフォルト値にフォールバックしなければならない
- **FR-017**: システムはLLMプロバイダーの認証情報を環境変数から取得しなければならない（例：OPENAI_API_KEY、ANTHROPIC_API_KEY）。TOML設定ファイルに認証情報を含めてはならない
- **FR-018**: システムは特定の指標で指定されたLLMプロバイダーが利用不可能（APIキー未設定、ネットワークエラー等）な場合、即座にエラーを返し評価を中断しなければならない
- **FR-019**: システムはLLMパラメータ（model、system_instruction、temperature、max_tokens、max_retries）について、メトリクスレベルの設定を最優先し、未指定の場合は`[llm_default]`セクションにフォールバック、それもない場合はハードコードされたデフォルト値（model: "anthropic:claude-sonnet-4-5-20250929", temperature: 0.0, max_tokens: None, max_retries: 3）を使用しなければならない
- **FR-020**: システムはTOML設定でクラス名のみを指定することで評価指標を識別し、`src/mixseek/evaluator/metrics/`ディレクトリから該当クラスを自動的に検索・ロードしなければならない。指定されたクラス名が見つからない場合は設定ロード時にエラーを返す

### 主要エンティティ *(機能がデータを含む場合に含める)*

すべてのエンティティはPydanticモデルとして `src/mixseek/models/` 配下に定義される（FR-006）：

- **EvaluationRequest（評価リクエスト）**: Pydanticモデル。フィールド: user_query（str）、submission（str）、team_id（str）、config（Optional[EvaluationConfig]）
- **EvaluationResult（評価結果）**: Pydanticモデル。フィールド: metrics（List[MetricScore]）、overall_score（float、0-100の範囲）
- **EvaluationConfig（評価設定）**: Pydanticモデル。`{workspace}/configs/evaluator.toml` からロード。フィールド: metric_weights（Dict[str, float]）、enabled_metrics（List[str]）、custom_metrics（Optional[Dict]）、llm_default（Optional[LLMDefaultConfig]、`[llm_default]`セクションに対応）
- **LLMDefaultConfig（LLMデフォルト設定）**: `[llm_default]`セクションに対応。フィールド: model（Optional[str]、デフォルト: "anthropic:claude-sonnet-4-5-20250929"）、temperature（Optional[float]、デフォルト: 0.0）、max_tokens（Optional[int]、デフォルト: None）、max_retries（Optional[int]、デフォルト: 3）
- **MetricConfig（指標設定）**: TOMLの[[metrics]]配列要素に対応。フィールド: name（str）、weight（float）、model（Optional[str]、`"provider:model-name"`形式）、system_instruction（Optional[str]）、temperature（Optional[float]）、max_tokens（Optional[int]）、max_retries（Optional[int]）。各LLMパラメータは、指定時はその値を使用、未指定時は`[llm_default]`にフォールバック、それもない場合はハードコードされたデフォルト値を使用（FR-019）
- **MetricScore（指標スコア）**: Pydanticモデル。フィールド: metric_name（str）、score（float、0-100の範囲）、evaluator_comment（str）

## 成功基準 *(必須)*

### 測定可能な成果

- **SC-001**: Evaluatorは一般的な入力（2000文字未満）に対し、30秒以内でAI応答を評価し品質スコアを受け取ることができる
- **SC-002**: 組み込み指標は同一の入力を複数回評価した場合、5%未満の分散で一貫したスコアを生成する
- **SC-003**: TOMLファイル経由の設定変更はシステムの再起動なしに即座に有効になる
- **SC-004**: カスタム指標は5分未満の開発時間で評価パイプラインに統合され機能するようになる
- **SC-005**: リトライメカニズムにより、一時的なLLM APIエラーから回復し、全ての評価指標が正常に結果を生成できる

## 前提

- ユーザはTOML設定ファイルにアクセスでき、基本的な設定構文を理解している
- AI応答は主にテキストベースで、選択されたLLM判定者がサポートする言語である
- 評価は単一のクエリ-応答ペアに対して実行される（バッチ評価はスコープ外）
- カスタム指標を必要とするユーザはPython開発能力を持っている
- LLM-as-a-Judge API呼び出しのためのネットワーク接続が利用可能である
- 標準的な品質指標（ClarityCoherence、Coverage、Relevance、LLMPlain）はほとんどの研究ユースケースに十分である
- 使用するLLMプロバイダーのAPIキーが適切な環境変数に設定されている（例：OPENAI_API_KEY、ANTHROPIC_API_KEY）

## 依存関係

- 親仕様: specs/001-specs
- LLMベース評価用のPydantic AI Direct Model Request API
  - **実装参照**: `specs/006-evaluator/assets/docs/pydantic-ai/` 配下のドキュメントを参照して正確な実装を行うこと
  - 主要ドキュメント: `direct.md`（Direct Model Request API）、`output.md`（構造化出力）、`message-history.md`（メッセージ履歴）
- `system_instruction` は、Pydantic AI Agent の `instructions` を示す用語である
- 設定管理用のTOMLパーシング機能
- 判定者ベース評価用のLLMサービスへのアクセス

## 実装構造

- **実装ディレクトリ**: `src/mixseek/evaluator/` 配下に評価ロジック、LLM呼び出し、指標実装を配置
- **データモデル**: `src/mixseek/models/` 配下に評価関連のPydanticモデルを配置（既存のconfig.py、result.pyと同様の構造）
- **設定ファイル**: ワークスペースディレクトリ直下の `configs/evaluator.toml` に評価器の設定を配置（指標重み、LLMモデル設定等）