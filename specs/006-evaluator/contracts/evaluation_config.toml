# Evaluator Configuration Example
# Location: {workspace}/configs/evaluator.toml

# Global default LLM model (fallback for metrics without explicit model)
# Format: "provider:model-name"
# Supported providers: anthropic, openai
default_model = "anthropic:claude-4-5-sonnet"

# Maximum retry attempts for LLM API calls (per metric)
# When an LLM API call fails, the system will retry up to this many times
# before raising an exception (FR-010)
max_retries = 3

# Built-in evaluation metrics
# Each metric must specify: name, weight
# Optional: model (overrides default_model for this specific metric)

[[metrics]]
# Metric name: Must be unique
name = "clarity_coherence"

# Weight: Must be between 0.0 and 1.0
# All metric weights must sum to 1.0 (FR-009)
weight = 0.4

# Optional: Override LLM model for this metric
# If not specified, uses default_model
model = "anthropic:claude-4-5-sonnet"

[[metrics]]
name = "coverage"
weight = 0.3
# No model specified - will use default_model

[[metrics]]
name = "relevance"
weight = 0.3
# Using a different model for this metric
model = "openai:gpt-5"

# Notes:
# - Weights must sum to 1.0 (within Â±0.001 tolerance)
# - Each metric name must be unique
# - Model format must be "provider:model-name"
# - API keys must be set in environment variables:
#   - ANTHROPIC_API_KEY for Anthropic models
#   - OPENAI_API_KEY for OpenAI models
