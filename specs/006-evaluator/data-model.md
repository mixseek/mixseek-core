# Data Model: AI Agent Output Evaluator

**Branch**: `022-mixseek-core-evaluator`
**Date**: 2025-10-22
**Phase**: Phase 1 - Design

## Overview

This document defines the data models for the AI Agent Output Evaluator component. All models are implemented as Pydantic v2 models for type safety, validation, and serialization. Models are organized into three categories:
1. **Request Models**: Input to the evaluation system
2. **Result Models**: Output from the evaluation system
3. **Configuration Models**: System configuration loaded from TOML files

---

## Model Hierarchy

```
Evaluation System
├── Request Models
│   └── EvaluationRequest
├── Result Models
│   ├── MetricScore
│   └── EvaluationResult
└── Configuration Models
    ├── MetricConfig
    └── EvaluationConfig
```

---

## 1. Request Models

### EvaluationRequest

**Purpose**: Encapsulates all inputs required for evaluating an AI agent's response.

**File**: `src/mixseek/models/evaluation_request.py`

```python
"""Evaluation request models."""

from pydantic import BaseModel, Field, field_validator
from typing import Optional


class EvaluationRequest(BaseModel):
    """
    Request for evaluating an AI agent's response.

    This model represents the input to the Evaluator component,
    containing the user's query, the AI agent's response, and
    contextual information for the evaluation.

    Attributes:
        user_query: The original query from the user
        submission: The response generated by the AI agent
        team_id: Identifier for the team that generated the response
        config: Optional custom evaluation configuration (overrides default)

    Example:
        ```python
        from mixseek.models.evaluation_request import EvaluationRequest

        request = EvaluationRequest(
            user_query="What are the benefits of Python?",
            submission="Python is a versatile programming language...",
            team_id="team-alpha-001"
        )
        ```

    Validation Rules:
        - user_query: Cannot be empty or whitespace-only (FR-013)
        - submission: Cannot be empty or whitespace-only (FR-013)
        - team_id: Must be non-empty string
    """

    user_query: str = Field(
        ...,
        description="The original query from the user",
        min_length=1,
        examples=["What are the benefits of Python?"]
    )

    submission: str = Field(
        ...,
        description="The response generated by the AI agent to be evaluated",
        min_length=1,
        examples=["Python is a versatile programming language..."]
    )

    team_id: str = Field(
        ...,
        description="Identifier for the team that generated this response",
        min_length=1,
        examples=["team-alpha-001"]
    )

    config: Optional['EvaluationConfig'] = Field(
        None,
        description="Optional custom evaluation configuration (overrides default from TOML)"
    )

    @field_validator("user_query")
    @classmethod
    def validate_user_query_not_empty(cls, v: str) -> str:
        """
        Validate that user query is not empty or whitespace-only.

        Raises:
            ValueError: If query is empty or whitespace-only (FR-013)
        """
        if not v or not v.strip():
            raise ValueError(
                "User query cannot be empty or whitespace-only. "
                "Please provide a valid query for evaluation."
            )
        return v.strip()

    @field_validator("submission")
    @classmethod
    def validate_submission_not_empty(cls, v: str) -> str:
        """
        Validate that AI response is not empty or whitespace-only.

        Raises:
            ValueError: If response is empty or whitespace-only (FR-013)
        """
        if not v or not v.strip():
            raise ValueError(
                "AI response cannot be empty or whitespace-only. "
                "Cannot evaluate empty responses."
            )
        return v.strip()

    @field_validator("team_id")
    @classmethod
    def validate_team_id_not_empty(cls, v: str) -> str:
        """Validate that team ID is not empty."""
        if not v or not v.strip():
            raise ValueError("Team ID cannot be empty")
        return v.strip()

    model_config = {
        "json_schema_extra": {
            "examples": [
                {
                    "user_query": "What are the key features of Python 3.13?",
                    "submission": "Python 3.13 introduces several key features including...",
                    "team_id": "team-alpha-001",
                    "config": None
                }
            ]
        }
    }
```

---

## 2. Result Models

### MetricScore

**Purpose**: Represents the evaluation result for a single metric (clarity_coherence, coverage, or relevance).

**File**: `src/mixseek/models/evaluation_result.py`

```python
"""Evaluation result models."""

from pydantic import BaseModel, Field, field_validator
from typing import List


class MetricScore(BaseModel):
    """
    Evaluation score for a single metric.

    This model represents the result of evaluating one specific metric
    (e.g., clarity_coherence, coverage, or relevance). It includes both
    a quantitative score and a qualitative comment explaining the evaluation.

    Attributes:
        metric_name: Name of the metric (e.g., "clarity_coherence", "coverage", "relevance")
        score: Numerical score (any real value). Built-in LLMJudgeMetrics return 0-100,
            but custom metrics may return negative values or values above 100
        evaluator_comment: Detailed explanation of the score (FR-012)

    Example:
        ```python
        from mixseek.models.evaluation_result import MetricScore

        # LLMJudgeMetric example (typically 0-100)
        metric = MetricScore(
            metric_name="clarity_coherence",
            score=85.5,
            evaluator_comment="The response is well-structured and easy to understand..."
        )

        # Custom metric example (arbitrary value)
        custom_metric = MetricScore(
            metric_name="performance_delta",
            score=-15.2,
            evaluator_comment="Performance degraded by 15.2%"
        )
        ```

    Validation Rules:
        - metric_name: Must be non-empty string
        - score: Any real value (no constraints)
        - evaluator_comment: Must be non-empty string
    """

    metric_name: str = Field(
        ...,
        description="Name of the evaluation metric",
        examples=["clarity_coherence", "coverage", "relevance"]
    )

    score: float = Field(
        ...,
        description="Numerical score (any real value, custom metrics may use negative values or values above 100)",
        examples=[85.5, 72.3, 91.0, -15.2, 150.0]
    )

    evaluator_comment: str = Field(
        ...,
        description="Detailed explanation of the score (FR-012)",
        min_length=1,
        examples=[
            "The response is well-structured and easy to understand. "
            "Technical terms are explained clearly. Minor improvements possible in conclusion."
        ]
    )

    @field_validator("metric_name")
    @classmethod
    def validate_metric_name(cls, v: str) -> str:
        """Validate that metric name is not empty."""
        if not v or not v.strip():
            raise ValueError("Metric name cannot be empty")
        return v.strip()

    @field_validator("score")
    @classmethod
    def round_score_to_two_decimals(cls, v: float) -> float:
        """Round score to 2 decimal places for consistency."""
        return round(v, 2)

    @field_validator("evaluator_comment")
    @classmethod
    def validate_comment_not_empty(cls, v: str) -> str:
        """Validate that comment is not empty."""
        if not v or not v.strip():
            raise ValueError("Evaluator comment cannot be empty")
        return v.strip()

    model_config = {
        "json_schema_extra": {
            "examples": [
                {
                    "metric_name": "clarity_coherence",
                    "score": 85.5,
                    "evaluator_comment": "The response is well-structured with clear language. "
                                        "Technical terms are explained adequately. "
                                        "Minor improvements possible in the conclusion section."
                }
            ]
        }
    }
```

### EvaluationResult

**Purpose**: Aggregates results from all evaluated metrics and provides an overall score.

**File**: `src/mixseek/models/evaluation_result.py` (continued)

```python
class EvaluationResult(BaseModel):
    """
    Complete evaluation result containing all metric scores and overall score.

    This model represents the final output of the evaluation system,
    containing individual metric scores and an aggregated overall score
    computed as a weighted average (FR-004, FR-012).

    Attributes:
        metrics: List of individual metric scores
        overall_score: Weighted average of all metric scores (any real value).
            Typically 0-100 when using only LLMJudgeMetrics, but may be negative
            or above 100 when custom metrics are included

    Example:
        ```python
        from mixseek.models.evaluation_result import EvaluationResult, MetricScore

        # LLMJudgeMetrics only example (typically 0-100)
        result = EvaluationResult(
            metrics=[
                MetricScore(metric_name="clarity_coherence", score=85.5, evaluator_comment="Clear..."),
                MetricScore(metric_name="relevance", score=90.0, evaluator_comment="Highly relevant..."),
            ],
            overall_score=87.2
        )

        # With custom metrics example (arbitrary value)
        result_with_custom = EvaluationResult(
            metrics=[
                MetricScore(metric_name="clarity_coherence", score=85.5, evaluator_comment="Clear..."),
                MetricScore(metric_name="performance_delta", score=-20.0, evaluator_comment="Degraded..."),
            ],
            overall_score=32.75  # Weighted average
        )
        ```

    Validation Rules:
        - metrics: Must contain at least 1 metric (FR-001 requires 3 built-in metrics)
        - overall_score: Any real value (no constraints)
        - overall_score: Should match weighted average of metric scores
    """

    metrics: List[MetricScore] = Field(
        ...,
        description="List of individual metric scores (FR-012)",
        min_length=1
    )

    overall_score: float = Field(
        ...,
        description="Weighted average of all metric scores (any real value)",
        examples=[87.2, 75.5, 92.3, -5.3, 120.0]
    )

    @field_validator("overall_score")
    @classmethod
    def round_overall_score(cls, v: float) -> float:
        """Round overall score to 2 decimal places."""
        return round(v, 2)

    @field_validator("metrics")
    @classmethod
    def validate_unique_metric_names(cls, v: List[MetricScore]) -> List[MetricScore]:
        """Validate that each metric appears only once."""
        metric_names = [m.metric_name for m in v]
        duplicates = [name for name in metric_names if metric_names.count(name) > 1]

        if duplicates:
            raise ValueError(
                f"Duplicate metric names found: {', '.join(set(duplicates))}. "
                "Each metric should appear only once in the result."
            )

        return v

    model_config = {
        "json_schema_extra": {
            "examples": [
                {
                    "metrics": [
                        {
                            "metric_name": "clarity_coherence",
                            "score": 85.5,
                            "evaluator_comment": "Well-structured and clear."
                        },
                        {
                            "metric_name": "coverage",
                            "score": 78.0,
                            "evaluator_comment": "Covers main points, some depth missing."
                        },
                        {
                            "metric_name": "relevance",
                            "score": 92.0,
                            "evaluator_comment": "Highly relevant to the query."
                        }
                    ],
                    "overall_score": 85.2
                }
            ]
        }
    }
```

---

## 3. Configuration Models

### MetricConfig

**Purpose**: Configuration for a single evaluation metric, loaded from TOML file.

**File**: `src/mixseek/models/evaluation_config.py`

```python
"""Evaluation configuration models."""

import tomllib
from pathlib import Path
from typing import Optional
from pydantic import BaseModel, Field, field_validator, model_validator


class MetricConfig(BaseModel):
    """
    Configuration for a single evaluation metric.

    This model corresponds to a [[metric]] section in the evaluator.toml file.
    Each metric can have its own weight and optionally its own LLM model.

    Attributes:
        name: Metric name (e.g., "clarity_coherence", "coverage", "relevance")
        weight: Metric weight for weighted average calculation (0.0 to 1.0) (FR-003)
        model: Optional LLM model override (format: "provider:model-name") (FR-015)

    Example TOML:
        ```toml
        [[metric]]
        name = "clarity_coherence"
        weight = 0.4
        model = "anthropic:claude-4-5-sonnet"
        ```

    Example Python:
        ```python
        from mixseek.models.evaluation_config import MetricConfig

        metric = MetricConfig(
            name="clarity_coherence",
            weight=0.4,
            model="anthropic:claude-4-5-sonnet"
        )
        ```

    Validation Rules:
        - name: Cannot be empty
        - weight: Must be between 0.0 and 1.0 (FR-003)
        - model: If provided, must be in "provider:model-name" format
    """

    name: str = Field(
        ...,
        description="Metric name (e.g., 'clarity_coherence', 'coverage', 'relevance')",
        examples=["clarity_coherence", "coverage", "relevance"]
    )

    weight: float = Field(
        ...,
        ge=0.0,
        le=1.0,
        description="Metric weight for weighted average (0.0 to 1.0) (FR-003)",
        examples=[0.4, 0.3, 0.3]
    )

    model: Optional[str] = Field(
        None,
        description="Optional LLM model override (format: 'provider:model-name') (FR-015)",
        examples=["anthropic:claude-4-5-sonnet", "openai:gpt-5"]
    )

    @field_validator("name")
    @classmethod
    def validate_name_not_empty(cls, v: str) -> str:
        """Validate that metric name is not empty."""
        if not v or not v.strip():
            raise ValueError("Metric name cannot be empty")
        return v.strip()

    @field_validator("model")
    @classmethod
    def validate_model_format(cls, v: Optional[str]) -> Optional[str]:
        """
        Validate that model format is 'provider:model-name'.

        Raises:
            ValueError: If model format is invalid
        """
        if v is not None:
            if ":" not in v:
                raise ValueError(
                    f"Invalid model format: '{v}'. "
                    "Expected format: 'provider:model-name' "
                    "(e.g., 'anthropic:claude-4-5-sonnet', 'openai:gpt-5')"
                )

            provider, model_name = v.split(":", 1)
            if not provider or not model_name:
                raise ValueError(
                    f"Invalid model format: '{v}'. "
                    "Both provider and model name must be non-empty."
                )

        return v

    model_config = {
        "json_schema_extra": {
            "examples": [
                {
                    "name": "clarity_coherence",
                    "weight": 0.4,
                    "model": "anthropic:claude-4-5-sonnet"
                },
                {
                    "name": "coverage",
                    "weight": 0.3,
                    "model": None  # Uses default_model
                }
            ]
        }
    }
```

### EvaluationConfig

**Purpose**: Global evaluation configuration, loaded from `{workspace}/configs/evaluator.toml`.

**File**: `src/mixseek/models/evaluation_config.py` (continued)

```python
class EvaluationConfig(BaseModel):
    """
    Global evaluation configuration loaded from {workspace}/configs/evaluator.toml.

    This model represents the complete evaluator configuration including
    default LLM model, retry settings, and all metric configurations.

    Attributes:
        default_model: Default LLM model for metrics without explicit model (FR-016)
        max_retries: Maximum retry attempts for LLM API calls (FR-010)
        metric: List of metric configurations

    Example TOML:
        ```toml
        default_model = "anthropic:claude-4-5-sonnet"
        max_retries = 3

        [[metric]]
        name = "clarity_coherence"
        weight = 0.4

        [[metric]]
        name = "coverage"
        weight = 0.3

        [[metric]]
        name = "relevance"
        weight = 0.3
        ```

    Example Python:
        ```python
        from pathlib import Path
        from mixseek.models.evaluation_config import EvaluationConfig

        workspace = Path("/path/to/workspace")
        config = EvaluationConfig.from_toml_file(workspace)

        print(f"Default model: {config.default_model}")
        print(f"Metrics: {len(config.metric)}")
        ```

    Validation Rules:
        - default_model: Must be in "provider:model-name" format (FR-016)
        - max_retries: Must be non-negative (FR-010)
        - metric: Must contain at least 1 metric
        - metric weights: Must sum to 1.0 (±0.001 tolerance) (FR-009)
        - metric names: Must be unique
    """

    default_model: str = Field(
        "anthropic:claude-4-5-sonnet",
        description="Default LLM model for metrics without explicit model (FR-016)",
        examples=["anthropic:claude-4-5-sonnet", "openai:gpt-5"]
    )

    max_retries: int = Field(
        3,
        ge=0,
        description="Maximum retry attempts for LLM API calls (FR-010)",
        examples=[3, 5, 10]
    )

    metric: List[MetricConfig] = Field(
        ...,
        description="List of metric configurations (FR-003)",
        min_length=1
    )

    @field_validator("default_model")
    @classmethod
    def validate_default_model_format(cls, v: str) -> str:
        """
        Validate that default model format is 'provider:model-name'.

        Raises:
            ValueError: If model format is invalid (FR-016)
        """
        if ":" not in v:
            raise ValueError(
                f"Invalid model format: '{v}'. "
                "Expected format: 'provider:model-name' "
                "(e.g., 'anthropic:claude-4-5-sonnet')"
            )

        provider, model_name = v.split(":", 1)
        if not provider or not model_name:
            raise ValueError(
                f"Invalid model format: '{v}'. "
                "Both provider and model name must be non-empty."
            )

        return v

    @model_validator(mode="after")
    def validate_weights_sum_to_one(self) -> "EvaluationConfig":
        """
        Validate that all metric weights sum to 1.0 (FR-009).

        Allows small floating-point tolerance (±0.001) to account for
        floating-point precision issues.

        Raises:
            ValueError: If weights don't sum to 1.0
        """
        total_weight = sum(metric.weight for metric in self.metric)

        if not (0.999 <= total_weight <= 1.001):
            weight_details = ', '.join(f'{m.name}={m.weight}' for m in self.metric)
            raise ValueError(
                f"Metric weights must sum to 1.0, got: {total_weight:.4f}. "
                f"Current weights: {weight_details}. "
                "Please adjust the weights in your configuration file."
            )

        return self

    @model_validator(mode="after")
    def validate_unique_metric_names(self) -> "EvaluationConfig":
        """
        Validate that all metric names are unique.

        Raises:
            ValueError: If duplicate metric names are found
        """
        names = [metric.name for metric in self.metric]
        duplicates = [name for name in names if names.count(name) > 1]

        if duplicates:
            raise ValueError(
                f"Duplicate metric names found: {', '.join(set(duplicates))}. "
                "Each metric must have a unique name."
            )

        return self

    @classmethod
    def from_toml_file(cls, workspace_path: Path) -> "EvaluationConfig":
        """
        Load configuration from TOML file.

        Args:
            workspace_path: Path to workspace root directory

        Returns:
            Validated EvaluationConfig instance

        Raises:
            FileNotFoundError: If evaluator.toml doesn't exist
            ValueError: If TOML is invalid or validation fails

        Example:
            ```python
            from pathlib import Path
            config = EvaluationConfig.from_toml_file(Path("/workspace"))
            ```
        """
        config_file = workspace_path / "configs" / "evaluator.toml"

        if not config_file.exists():
            raise FileNotFoundError(
                f"Evaluator configuration not found: {config_file}\n"
                f"Please create {config_file} with your evaluation settings.\n"
                "See documentation for example configuration."
            )

        try:
            with open(config_file, "rb") as f:
                config_data = tomllib.load(f)
        except tomllib.TOMLDecodeError as e:
            raise ValueError(
                f"Invalid TOML syntax in {config_file}: {e}\n"
                "Please check your configuration file for syntax errors."
            ) from e

        return cls.model_validate(config_data)

    def get_model_for_metric(self, metric_name: str) -> str:
        """
        Get LLM model for a specific metric.

        Returns metric-specific model if set, otherwise returns default_model.

        Args:
            metric_name: Name of the metric

        Returns:
            Model identifier in "provider:model-name" format

        Raises:
            ValueError: If metric not found in configuration

        Example:
            ```python
            config = EvaluationConfig.from_toml_file(workspace)
            clarity_coherence_model = config.get_model_for_metric("clarity_coherence")
            ```
        """
        for metric in self.metric:
            if metric.name == metric_name:
                return metric.model if metric.model else self.default_model

        raise ValueError(
            f"Metric not found: {metric_name}. "
            f"Available metrics: {', '.join(m.name for m in self.metric)}"
        )

    model_config = {
        "json_schema_extra": {
            "examples": [
                {
                    "default_model": "anthropic:claude-4-5-sonnet",
                    "max_retries": 3,
                    "metric": [
                        {"name": "clarity_coherence", "weight": 0.4, "model": None},
                        {"name": "coverage", "weight": 0.3, "model": None},
                        {"name": "relevance", "weight": 0.3, "model": "openai:gpt-5"}
                    ]
                }
            ]
        }
    }
```

---

## Relationship Diagram

```
┌─────────────────────────────────────────────────────────────┐
│                    Evaluation Flow                          │
└─────────────────────────────────────────────────────────────┘

Input:
┌───────────────────┐
│ EvaluationRequest │
├───────────────────┤
│ user_query        │
│ submission       │
│ team_id           │
│ config (optional) │
└────────┬──────────┘
         │
         ├──► uses ──► ┌─────────────────┐
         │             │ EvaluationConfig│
         │             ├─────────────────┤
         │             │ default_model   │
         │             │ max_retries     │
         │             │ metric[]        │
         │             └────────┬────────┘
         │                      │
         │                      │ contains
         │                      ▼
         │             ┌─────────────────┐
         │             │  MetricConfig   │
         │             ├─────────────────┤
         │             │ name            │
         │             │ weight          │
         │             │ model           │
         │             └─────────────────┘
         │
         ▼
┌──────────────────┐
│   Evaluator      │
│ (processing)     │
└────────┬─────────┘
         │
         │ produces
         ▼

Output:
┌───────────────────┐
│ EvaluationResult  │
├───────────────────┤
│ metrics[]         │◄──┐
│ overall_score     │   │
└───────────────────┘   │
                        │ contains
                        │
              ┌─────────┴────────┐
              │   MetricScore    │
              ├──────────────────┤
              │ metric_name      │
              │ score            │
              │ evaluator_comment│
              └──────────────────┘
```

---

## Validation Summary

### Field-Level Validations

| Model | Field | Validation |
|-------|-------|------------|
| EvaluationRequest | user_query | Not empty/whitespace (FR-013) |
| EvaluationRequest | submission | Not empty/whitespace (FR-013) |
| EvaluationRequest | team_id | Not empty |
| MetricScore | metric_name | Not empty |
| MetricScore | score | 0-100 range, rounded to 2 decimals (FR-002) |
| MetricScore | evaluator_comment | Not empty (FR-012) |
| EvaluationResult | metrics | Min 1 item, unique names |
| EvaluationResult | overall_score | 0-100 range, rounded to 2 decimals (FR-004) |
| MetricConfig | name | Not empty |
| MetricConfig | weight | 0.0-1.0 range (FR-003) |
| MetricConfig | model | "provider:model-name" format if set (FR-015) |
| EvaluationConfig | default_model | "provider:model-name" format (FR-016) |
| EvaluationConfig | max_retries | Non-negative (FR-010) |

### Model-Level Validations

| Model | Validation | Requirement |
|-------|------------|-------------|
| EvaluationResult | Unique metric names | Each metric appears once |
| EvaluationConfig | Weights sum to 1.0 (±0.001) | FR-009 |
| EvaluationConfig | Unique metric names | No duplicates |

---

## Type Safety Benefits

1. **IDE Autocomplete**: Full type hints enable intelligent code completion
2. **Static Type Checking**: mypy/pyright can catch type errors before runtime
3. **Runtime Validation**: Pydantic validates all data at runtime
4. **Serialization**: Automatic JSON/dict conversion with `.model_dump()` and `.model_dump_json()`
5. **Documentation**: Models serve as living documentation with examples

---

## Usage Examples

### Loading Configuration

```python
from pathlib import Path
from mixseek.models.evaluation_config import EvaluationConfig

workspace = Path("/workspace")
config = EvaluationConfig.from_toml_file(workspace)

print(f"Loaded {len(config.metric)} metrics")
for metric in config.metric:
    model = metric.model or config.default_model
    print(f"  {metric.name}: weight={metric.weight}, model={model}")
```

### Creating Evaluation Request

```python
from mixseek.models.evaluation_request import EvaluationRequest

request = EvaluationRequest(
    user_query="What are the benefits of Python?",
    submission="Python offers simplicity, readability, and a vast ecosystem...",
    team_id="team-001"
)
```

### Building Evaluation Result

```python
from mixseek.models.evaluation_result import EvaluationResult, MetricScore

result = EvaluationResult(
    metrics=[
        MetricScore(
            metric_name="clarity_coherence",
            score=85.5,
            evaluator_comment="Well-structured and clear"
        ),
        MetricScore(
            metric_name="coverage",
            score=78.0,
            evaluator_comment="Covers main points"
        ),
        MetricScore(
            metric_name="relevance",
            score=92.0,
            evaluator_comment="Highly relevant"
        )
    ],
    overall_score=85.2  # Weighted average
)

# Serialize to JSON
json_output = result.model_dump_json(indent=2)
```

---

## Implementation Location

All models will be implemented in:

```
src/mixseek/models/
├── evaluation_request.py      # EvaluationRequest
├── evaluation_result.py       # MetricScore, EvaluationResult
└── evaluation_config.py       # MetricConfig, EvaluationConfig
```

This follows the existing pattern in `src/mixseek/models/` and aligns with Article 1 (Library-First Principle) and Article 12 (Documentation Standards).

---

**Document Version**: 1.0
**Status**: Complete
**Next**: Generate contracts/ directory with JSON schemas
